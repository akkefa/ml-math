{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Neural Networks Work\n",
    "\n",
    "An MLP consists of multiple layers of neurons: an input layer, one or more hidden layers, and an output layer. Each neuron in one layer is connected to all neurons in the next layer. The network transforms inputs through these layers using a series of linear and nonlinear functions to predict the target output.\n",
    "\n",
    "## Structure of  Neural Network\n",
    "For an MLP with:\n",
    "- Input layer: $ X = [x_1, x_2, \\dots, x_n] $\n",
    "- Hidden layers: With activations $ a^{(l)} $ and weights $ W^{(l)} $ and biases $ b^{(l)} $, where $ l $ denotes the layer.\n",
    "- Output layer: $ \\hat{y} = f(a^{(L)}) $, where $ L $ is the final layer, and $ f $ is the activation function.\n",
    "\n",
    "The MLP performs the following operations:\n",
    "- Linear transformation: $ z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)} $\n",
    "- Nonlinear activation: $ a^{(l)} = \\sigma(z^{(l)}) $\n",
    "\n",
    "For the output layer:\n",
    "- Output: $ \\hat{y} = \\sigma(W^{(L)} a^{(L-1)} + b^{(L)}) $\n",
    "\n",
    "### Loss Function\n",
    "The loss $ \\mathcal{L} $ measures the difference between the predicted output $ \\hat{y} $ and the true label $ y $. A common loss function is Mean Squared Error (MSE) for regression or Cross-Entropy Loss for classification:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{2} (\\hat{y} - y)^2 \\quad \\text{(for regression)}\n",
    "$$\n",
    "\n",
    "### Back propagation Algorithm\n",
    "Back propagation updates the weights of the MLP using the gradient of the loss function with respect to the model’s parameters (weights and biases). To calculate these gradients, we use the **chain rule** of calculus.\n",
    "\n",
    "#### Chain Rule\n",
    "If a function $ y $ depends on $ u $, and $ u $ depends on $ x $, the derivative of $ y $ with respect to $ x $ can be found using:\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}\n",
    "$$\n",
    "In the MLP, backpropagation applies this rule to calculate the gradients layer by layer, starting from the output and propagating backward through the network.\n",
    "\n",
    "#### Steps in Backpropagation:\n",
    "1. **Forward Pass**: Compute activations for each layer, $ a^{(l)} $, and store them.\n",
    "2. **Loss Calculation**: Compute the loss $ \\mathcal{L}(\\hat{y}, y) $.\n",
    "3. **Backpropagate Errors**:\n",
    "   - Calculate the gradient of the loss with respect to the output $ \\hat{y} $.\n",
    "   - Propagate the error backwards using the chain rule to update the weights.\n",
    "\n",
    "The gradients are computed as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W^{(l)}} = \\delta^{(l)} (a^{(l-1)})^T\n",
    "$$\n",
    "\n",
    "where $ \\delta^{(l)} $ is the error term for layer $ l $, which is computed recursively:\n",
    "\n",
    "$$\n",
    "\\delta^{(L)} = \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\cdot \\sigma'(z^{(L)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\delta^{(l)} = (\\delta^{(l+1)} W^{(l+1)}) \\cdot \\sigma'(z^{(l)})\n",
    "$$\n",
    "\n",
    "4. **Update Weights**: After calculating the gradients, update the weights using gradient descent:\n",
    "   \n",
    "$$\n",
    "W^{(l)} = W^{(l)} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial W^{(l)}}\n",
    "$$\n",
    "where $ \\eta $ is the learning rate.\n",
    "\n",
    "### Toy Example\n",
    "\n",
    "Let's take a simple 2-layer MLP with one input, one hidden layer (2 neurons), and one output. The input is $ x = 1 $, the target is $ y = 0 $, and we use a sigmoid activation function.\n",
    "\n",
    "1. **Initialization**:\n",
    "   - Input to hidden weights: $ W^{(1)} = [[0.15, 0.2], [0.25, 0.3]] $\n",
    "   - Hidden to output weights: $ W^{(2)} = [0.4, 0.45] $\n",
    "   - Biases: $ b^{(1)} = [0.35], b^{(2)} = 0.6 $\n",
    "\n",
    "2. **Forward pass**:\n",
    "   \n",
    "   $$\n",
    "   z^{(1)} = W^{(1)} \\cdot x + b^{(1)} = [0.5, 0.6]\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   a^{(1)} = \\sigma(z^{(1)}) = [0.622, 0.645]\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   z^{(2)} = W^{(2)} \\cdot a^{(1)} + b^{(2)} = 1.105\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   \\hat{y} = \\sigma(z^{(2)}) = 0.751\n",
    "   $$\n",
    "\n",
    "3. **Loss calculation**:\n",
    "   \n",
    "   $$\n",
    "   \\mathcal{L} = \\frac{1}{2} (\\hat{y} - y)^2 = 0.282\n",
    "   $$\n",
    "\n",
    "4. **Backward pass**:\n",
    "   - Output layer error:\n",
    "  \n",
    "     $$\n",
    "     \\delta^{(2)} = (\\hat{y} - y) \\cdot \\sigma'(z^{(2)}) = 0.139\n",
    "     $$\n",
    "   - Hidden layer error:\n",
    "  \n",
    "     $$\n",
    "     \\delta^{(1)} = \\delta^{(2)} W^{(2)} \\cdot \\sigma'(z^{(1)}) = [0.013, 0.014]\n",
    "     $$\n",
    "\n",
    "5. **Update weights** (with learning rate $ \\eta = 0.5 $):\n",
    "   \n",
    "   $$\n",
    "   W^{(2)} = W^{(2)} - \\eta \\delta^{(2)} a^{(1)} = [0.37, 0.42]\n",
    "   $$\n",
    "   \n",
    "   $$\n",
    "   W^{(1)} = W^{(1)} - \\eta \\delta^{(1)} x = [[0.144, 0.19], [0.236, 0.286]]\n",
    "   $$\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.1016080379486084\n",
      "Epoch 100, Loss: 0.004436768125742674\n",
      "Epoch 200, Loss: 0.0020264554768800735\n",
      "Epoch 300, Loss: 0.0012776795774698257\n",
      "Epoch 400, Loss: 0.0009210538701154292\n",
      "Final prediction: 0.02673455886542797\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple MLP model\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 2)  # 1 input, 2 hidden neurons\n",
    "        self.fc2 = nn.Linear(2, 1)  # 2 hidden neurons, 1 output\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = SimpleMLP()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.5)\n",
    "\n",
    "# Sample data\n",
    "x = torch.tensor([[1.0]])  # Input\n",
    "y = torch.tensor([[0.0]])  # Target\n",
    "\n",
    "# Training loop\n",
    "epochs = 500\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    loss = criterion(output, y)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()  # Clear previous gradients\n",
    "    loss.backward()  # Compute gradients\n",
    "    optimizer.step()  # Update weights\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "# Final output\n",
    "print(f'Final prediction: {model(x).item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Computation Graph (Auto diff)?\n",
    "\n",
    " ```{image}  https://github.com/akkefa/ml-notes/releases/download/v0.1.0/computational_graph.png\n",
    ":align: center\n",
    ":alt: Computation Graph\n",
    ":width: 80%\n",
    "```\n",
    "\n",
    "Backpropagation calculates the gradient of the loss function with respect to each weight in the neural network. It does this by:\n",
    "1. **Forward Pass:** The network makes predictions, and the loss is computed.\n",
    "2. **Backward Pass (Backpropagation):** The gradient of the loss w.r.t. each parameter is computed by following the chain rule along the computation graph.\n",
    "\n",
    "### Key Concepts:\n",
    "- **Chain Rule:** The chain rule is used to compute the derivative of a composite function. If a variable $ z $ depends on $ y $, and $ y $ depends on $ x $, then:\n",
    "  \n",
    "  $$\n",
    "  \\frac{dz}{dx} = \\frac{dz}{dy} \\cdot \\frac{dy}{dx}\n",
    "  $$\n",
    "\n",
    "- **Computation Graph:** Backpropagation operates on a computation graph where:\n",
    "  - Each node represents an operation or variable.\n",
    "  - Arrows represent data dependencies.\n",
    "  \n",
    "  During backpropagation, the gradient flows backward through the edges, from the loss to each parameter.\n",
    "\n",
    "---\n",
    "\n",
    "### Detailed Example of Backpropagation with a Simple Neural Network\n",
    "\n",
    "Let’s consider a simple network with one input $ x $, one weight $ w $, and an output $ y $:\n",
    "\n",
    "$$\n",
    "y = w \\cdot x\n",
    "$$\n",
    "$$\n",
    "L = \\frac{1}{2} (y - t)^2\n",
    "$$\n",
    "\n",
    "Where $ t $ is the target value and $ L $ is the loss function.\n",
    "\n",
    "#### Forward Pass:\n",
    "1. Compute the output: \n",
    "   \n",
    "   $$\n",
    "   y = w \\cdot x\n",
    "   $$\n",
    "2. Compute the loss:\n",
    "   \n",
    "   $$\n",
    "   L = \\frac{1}{2} (y - t)^2\n",
    "   $$\n",
    "\n",
    "#### Backward Pass (Backpropagation):\n",
    "We want to compute the gradient of the loss $ L $ w.r.t the weight $ w $, i.e., $ \\frac{dL}{dw} $.\n",
    "\n",
    "Using the chain rule:\n",
    "1. Compute $ \\frac{dL}{dy} $:\n",
    "   \n",
    "   $$\n",
    "   \\frac{dL}{dy} = (y - t)\n",
    "   $$\n",
    "2. Compute $ \\frac{dy}{dw} $:\n",
    "   \n",
    "   $$\n",
    "   \\frac{dy}{dw} = x\n",
    "   $$\n",
    "3. Combine using the chain rule:\n",
    "   \n",
    "   $$\n",
    "   \\frac{dL}{dw} = \\frac{dL}{dy} \\cdot \\frac{dy}{dw} = (y - t) \\cdot x\n",
    "   $$\n",
    "\n",
    "This is the gradient that backpropagation computes and uses to update the weight $ w $ during training.\n",
    "\n",
    "---\n",
    "\n",
    "### Computation Graph for Backpropagation\n",
    "\n",
    "Let's build the computation graph for the above example:\n",
    "\n",
    "1. $ z1 = w \\cdot x $ (Multiplication)\n",
    "2. $ z2 = z1 - t $ (Subtraction)\n",
    "3. $ L = \\frac{1}{2} z2^2 $ (Squared error)\n",
    "\n",
    "Each node in the graph represents an operation or variable, and we can propagate gradients backward from the loss function to compute the gradients of all inputs.\n",
    "\n",
    "\n",
    "### Example in PyTorch: Backpropagation\n",
    "\n",
    "Let’s implement the above example using PyTorch's autograd:\n",
    "\n",
    "**Explanation:**\n",
    "1. We define a scalar $ x $, weight $ w $, and target $ t $.\n",
    "2. We compute the output $ y = w \\cdot x $ and the loss $ L = \\frac{1}{2}(y - t)^2 $.\n",
    "3. Using `loss.backward()`, PyTorch computes the gradient of the loss w.r.t. $ w $ via backpropagation. The result is stored in `w.grad`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of the loss with respect to w: -4.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Initialize input, weight, and target\n",
    "x = torch.tensor(2.0, requires_grad=False)\n",
    "w = torch.tensor(1.5, requires_grad=True)\n",
    "t = torch.tensor(5.0, requires_grad=False)\n",
    "\n",
    "# Forward pass: Compute output and loss\n",
    "y = w * x\n",
    "loss = 0.5 * (y - t)**2\n",
    "\n",
    "# Backward pass: Compute gradients\n",
    "loss.backward()\n",
    "\n",
    "# Output the gradient of the loss w.r.t. w\n",
    "print(f\"Gradient of the loss with respect to w: {w.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_notes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
