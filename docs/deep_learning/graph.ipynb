{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Networks\n",
    "\n",
    "Graph Neural Networks (GNNs) are a class of neural networks that operate on graphs. They are a powerful tool for solving problems in domains such as social network analysis, recommender systems, and combinatorial optimization.\n",
    "\n",
    "You can read about Graph data structures /algorithms/graphs.html#graphs-data-structure section.\n",
    "\n",
    "## Node Representations\n",
    "\n",
    "The goal of a GNN is to learn a function that maps a graph to a representation of its nodes. This representation can then be used for various downstream tasks, such as node classification, link prediction, and clustering.\n",
    "\n",
    "### DeepWalk\n",
    "\n",
    "DeepWalk is a simple algorithm for learning node representations in a graph. It works by performing random walks on the graph, and using the SkipGram model from Word2Vec to learn the embeddings of the nodes.\n",
    "\n",
    "It introduces important concepts such as embeddings that are at the core of GNNs. Unlike traditional neural networks, the goal of this architecture is to produce representations that are then fed to other models, which perform downstream tasks (for example, node classification).\n",
    "\n",
    "DeepWalk architecture and its two major components: **Word2Vec** and **random walks**.\n",
    "\n",
    "#### Word2Vec\n",
    "\n",
    "The first step to comprehending the DeepWalk algorithm is to understand its major component: Word2Vec.\n",
    "it proposed a new technique to translate words into vectors (also known as embeddings) using large datasets of text. These representations can then be used in downstream tasks, such as sentiment classification.\n",
    "\n",
    "One of the most surprising results of Word2Vec is its ability to solve analogies. A popular example is how it can answer the question “man is to woman, what king is to ___?” It can be calculated as follows:\n",
    "\n",
    "##### CBOW versus skip-gram\n",
    "its only goal is to produce high-quality embeddings.\n",
    "\n",
    "The continuous bag-of-words (CBOW) model:\n",
    "\n",
    "This is trained to predict a word using its surrounding context (words coming before and after the target word). The order of context words does not matter since their embeddings are summed in the model. The authors claim to obtain better results using four words before and after the one that is predicted.\n",
    "\n",
    "The continuous skip-gram model:\n",
    "\n",
    "Here, we feed a single word to the model and try to predict the words around it. Increasing the range of context words leads to better embeddings but also increases the training time.\n",
    "\n",
    "```{image} https://i.stack.imgur.com/ShJJX.png\n",
    ":alt: CBOW and skip-gram models\n",
    ":width: 80%\n",
    ":align: center\n",
    "```\n",
    "\n",
    "##### Creating skip-grams\n",
    "\n",
    "For now, we will focus on the skip-gram model since it is the architecture used by DeepWalk. Skip-grams are implemented as pairs of words with the following structure ( target word, context word). where target word is the input and context word is the word to predict. The number of skip grams for the same target word depends on a parameter called context size.\n",
    "\n",
    "\n",
    "| Context Size | Text | Skip-grams |\n",
    "| :---: | :---: | :---: |\n",
    "| 1 | the train was late. | ('the', 'train') |\n",
    "|  | the train was late | $\\begin{array}{l}\\text { ('train', 'the') } \\\\\\text { ('train', 'was') }\\end{array}$ |\n",
    "|  | the train was late | $\\begin{array}{l}\\text { ('was', 'train') } \\\\\\text { ('was', 'late') }\\end{array}$ |\n",
    "|  | the train was late | ('late', 'was') |\n",
    "| 2 | the train was late | $\\begin{array}{l}\\text { ('the', 'train') } \\\\\\text { ('the', 'was') }\\end{array}$ |\n",
    "|  | the train was late | $\\begin{array}{l}\\text { ('train', 'the') } \\\\\\text { ('train', 'was') } \\\\\\text { ('train', 'late') }\\end{array}$ |\n",
    "|  | the train was late | $\\begin{array}{l}\\text { ('was', 'the') } \\\\\\text { ('was', 'train') } \\\\\\text { ('was', 'late') }\\end{array}$ |\n",
    "|  | the train was late | $\\begin{array}{l}\\text { ('late', 'train') } \\\\\\text { ('late', 'was') }\\end{array}$ |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
