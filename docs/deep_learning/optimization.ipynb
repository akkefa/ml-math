{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Optimization\n",
    "\n",
    "## Gradient Descent Algorithm\n",
    "\n",
    "\n",
    "Gradient Descent is an optimization algorithm used to minimize a function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. It's widely used in machine learning to update parameters of models.\n",
    "\n",
    "Mathematical Explanation:\n",
    "\n",
    "Given a function $ f(\\theta) $ where $ \\theta $ represents the parameters, the goal is to find $ \\theta $ that minimizes $ f(\\theta) $.\n",
    "\n",
    "Update Rule:\n",
    "\n",
    "$$\n",
    "\\theta_{\\text{new}} = \\theta_{\\text{old}} - \\eta \\nabla_{\\theta} f(\\theta_{\\text{old}})\n",
    "$$\n",
    "\n",
    "- $ \\eta $ is the learning rate (a small positive number).\n",
    "- $ \\nabla_{\\theta} f(\\theta_{\\text{old}}) $ is the gradient of the function at $ \\theta_{\\text{old}} $.\n",
    "\n",
    "Visual Illustration:\n",
    "\n",
    "Imagine you're at the top of a hill (the maximum of the function), and you want to get to the bottom (the minimum). At each step, you look around for the steepest downward slope (the negative gradient) and take a step in that direction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Stochastic Gradient Descent is a variation of gradient descent where the gradient is estimated using a single sample (or a mini-batch) rather than the entire dataset. This makes it computationally efficient and allows it to handle large datasets.\n",
    "\n",
    "Update Rule:\n",
    "\n",
    "$$\n",
    "\\theta_{\\text{new}} = \\theta_{\\text{old}} - \\eta \\nabla_{\\theta} f(\\theta_{\\text{old}}; x_i, y_i)\n",
    "$$\n",
    "\n",
    "- $ (x_i, y_i) $ is a single data point.\n",
    "- The gradient $ \\nabla_{\\theta} f(\\theta_{\\text{old}}; x_i, y_i) $ is computed using only this data point.\n",
    "\n",
    "Benefits of SGD:\n",
    "\n",
    "- Faster iterations due to less computation per update.\n",
    "- Introduces noise that can help escape local minima.\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Linear Regression with Gradient Descent\n",
    "\n",
    "Suppose we have a dataset:\n",
    "\n",
    "| $ x_i $ | $ y_i $ |\n",
    "|-----------|-----------|\n",
    "|     1     |     2     |\n",
    "|     2     |     4     |\n",
    "|     3     |     6     |\n",
    "|     4     |     8     |\n",
    "\n",
    "We want to fit a linear model $ y = w x + b $ using gradient descent.\n",
    "\n",
    "Loss Function (Mean Squared Error):\n",
    "\n",
    "$$\n",
    "L(w, b) = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - (w x_i + b))^2\n",
    "$$\n",
    "\n",
    "Compute Gradients:\n",
    "\n",
    "- Gradient with respect to $ w $:\n",
    "\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial w} = -\\frac{2}{N} \\sum_{i=1}^{N} x_i (y_i - (w x_i + b))\n",
    "  $$\n",
    "\n",
    "- Gradient with respect to $ b $:\n",
    "\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial b} = -\\frac{2}{N} \\sum_{i=1}^{N} (y_i - (w x_i + b))\n",
    "  $$\n",
    "\n",
    "Update Rules:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "w_{\\text{new}} & = w_{\\text{old}} - \\eta \\frac{\\partial L}{\\partial w} \\\\\n",
    "b_{\\text{new}} & = b_{\\text{old}} - \\eta \\frac{\\partial L}{\\partial b}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Step-by-Step Calculation:\n",
    "\n",
    "Let's initialize $ w = 0 $, $ b = 0 $, and $ \\eta = 0.01 $.\n",
    "\n",
    "First Iteration:\n",
    "\n",
    "1. Compute predictions:\n",
    "\n",
    "   $$\n",
    "   \\hat{y}_i = w x_i + b = 0 \\times x_i + 0 = 0\n",
    "   $$\n",
    "\n",
    "2. Compute gradients:\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial w} = -\\frac{2}{4} \\sum_{i=1}^{4} x_i (y_i - \\hat{y}_i) = -\\frac{1}{2} \\sum_{i=1}^{4} x_i y_i\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial b} = -\\frac{2}{4} \\sum_{i=1}^{4} (y_i - \\hat{y}_i) = -\\frac{1}{2} \\sum_{i=1}^{4} y_i\n",
    "   $$\n",
    "\n",
    "3. Calculate sums:\n",
    "\n",
    "   $$\n",
    "   \\sum_{i=1}^{4} x_i y_i = 1 \\times 2 + 2 \\times 4 + 3 \\times 6 + 4 \\times 8 = 60\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   \\sum_{i=1}^{4} y_i = 2 + 4 + 6 + 8 = 20\n",
    "   $$\n",
    "\n",
    "4. Compute gradients:\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial w} = -\\frac{1}{2} \\times 60 = -30\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial b} = -\\frac{1}{2} \\times 20 = -10\n",
    "   $$\n",
    "\n",
    "5. Update parameters:\n",
    "\n",
    "   $$\n",
    "   w_{\\text{new}} = 0 - 0.01 \\times (-30) = 0 + 0.3 = 0.3\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   b_{\\text{new}} = 0 - 0.01 \\times (-10) = 0 + 0.1 = 0.1\n",
    "   $$\n",
    "\n",
    "Final Thoughts:\n",
    "\n",
    "- Gradient Descent is suitable for smaller datasets where computational efficiency is not a concern.\n",
    "- Stochastic Gradient Descent is better for larger datasets and can help in escaping local minima due to its stochastic nature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: w = 1.5104, b = 0.4936, Loss = 1.1751\n",
      "Epoch 20: w = 1.7583, b = 0.5584, Loss = 0.0843\n",
      "Epoch 30: w = 1.8030, b = 0.5547, Loss = 0.0529\n",
      "Epoch 40: w = 1.8149, b = 0.5404, Loss = 0.0492\n",
      "Epoch 50: w = 1.8213, b = 0.5248, Loss = 0.0463\n",
      "Epoch 60: w = 1.8267, b = 0.5093, Loss = 0.0436\n",
      "Epoch 70: w = 1.8319, b = 0.4943, Loss = 0.0410\n",
      "Epoch 80: w = 1.8369, b = 0.4797, Loss = 0.0387\n",
      "Epoch 90: w = 1.8417, b = 0.4655, Loss = 0.0364\n",
      "Epoch 100: w = 1.8463, b = 0.4518, Loss = 0.0343\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Data\n",
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "y = torch.tensor([2.0, 4.0, 6.0, 8.0])\n",
    "\n",
    "# Parameters\n",
    "w = torch.tensor(0.0, requires_grad=True)\n",
    "b = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "# Learning rate\n",
    "eta = 0.01\n",
    "\n",
    "# Number of epochs\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = w * x + b\n",
    "    # Compute and print loss\n",
    "    loss = ((y_pred - y) ** 2).mean()\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    # Update parameters\n",
    "    with torch.no_grad():\n",
    "        w -= eta * w.grad\n",
    "        b -= eta * b.grad\n",
    "    # Zero gradients\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "    # Print progress\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}: w = {w.item():.4f}, b = {b.item():.4f}, Loss = {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: w = 1.9437, b = 0.1718, Loss = 0.0049\n",
      "Epoch 20: w = 1.9503, b = 0.1516, Loss = 0.0038\n",
      "Epoch 30: w = 1.9554, b = 0.1333, Loss = 0.0030\n",
      "Epoch 40: w = 1.9613, b = 0.1178, Loss = 0.0023\n",
      "Epoch 50: w = 1.9648, b = 0.1036, Loss = 0.0018\n",
      "Epoch 60: w = 1.9693, b = 0.0914, Loss = 0.0014\n",
      "Epoch 70: w = 1.9733, b = 0.0807, Loss = 0.0011\n",
      "Epoch 80: w = 1.9762, b = 0.0711, Loss = 0.0008\n",
      "Epoch 90: w = 1.9792, b = 0.0628, Loss = 0.0007\n",
      "Epoch 100: w = 1.9809, b = 0.0551, Loss = 0.0005\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "# Data\n",
    "x = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n",
    "y = torch.tensor([[2.0], [4.0], [6.0], [8.0]])\n",
    "\n",
    "# Parameters\n",
    "w = torch.randn(1, 1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "\n",
    "# Learning rate\n",
    "eta = 0.01\n",
    "\n",
    "# Number of epochs\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    permutation = torch.randperm(x.size()[0])\n",
    "    for i in permutation:\n",
    "        xi = x[i]\n",
    "        yi = y[i]\n",
    "        # Forward pass\n",
    "        y_pred = xi @ w + b\n",
    "        # Compute loss\n",
    "        loss = (y_pred - yi).pow(2).mean()\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Update parameters\n",
    "        with torch.no_grad():\n",
    "            w -= eta * w.grad\n",
    "            b -= eta * b.grad\n",
    "        # Zero gradients\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "    # Print progress\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        # Compute total loss\n",
    "        y_pred = x @ w + b\n",
    "        loss = (y_pred - y).pow(2).mean()\n",
    "        print(f'Epoch {epoch+1}: w = {w.item():.4f}, b = {b.item():.4f}, Loss = {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced gradient descent methods\n",
    "While basic Gradient Descent and Stochastic Gradient Descent (SGD) are foundational optimization algorithms in machine learning, they have limitations such as slow convergence and sensitivity to the choice of learning rate. To address these issues, advanced optimization algorithms have been developed. These methods aim to accelerate convergence, adapt learning rates, and improve overall performance.\n",
    "\n",
    "Below, we'll explore several advanced gradient descent methods:\n",
    "\n",
    "- Learning Rate Scheduling\n",
    "- Nesterov Momentum\n",
    "- AdaGrad\n",
    "- AdaDelta\n",
    "- RMSprop\n",
    "- Adam\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Scheduling\n",
    "\n",
    "Learning rate scheduling involves changing the learning rate ($ \\eta $) during training to improve convergence. A high learning rate can help escape local minima, while a low learning rate can fine-tune the parameters.\n",
    "\n",
    "Common Schedules:\n",
    "\n",
    "- **Time-based Decay:**\n",
    "\n",
    "  $$\n",
    "  \\eta_t = \\frac{\\eta_0}{1 + k t}\n",
    "  $$\n",
    "\n",
    "- **Step Decay:**\n",
    "\n",
    "  Reduce $ \\eta $ by a factor every few epochs.\n",
    "\n",
    "- **Exponential Decay:**\n",
    "\n",
    "  $$\n",
    "  \\eta_t = \\eta_0 e^{-k t}\n",
    "  $$\n",
    "\n",
    "- **Cosine Annealing:**\n",
    "\n",
    "  Uses a cosine function to adjust $ \\eta $.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "We'll use the same linear regression problem as before but implement a time-based decay learning rate.\n",
    "\n",
    "- **Learning Rate Update:**\n",
    "\n",
    "  $$\n",
    "  \\eta_t = \\frac{\\eta_0}{1 + k t}\n",
    "  $$\n",
    "\n",
    "  - $ \\eta_0 $: Initial learning rate\n",
    "  - $ k $: Decay rate\n",
    "  - $ t $: Current epoch\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- We update the learning rate at each epoch based on the time-based decay formula.\n",
    "- The learning rate decreases over time, allowing larger steps initially and smaller steps as we approach the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Learning Rate: 0.0917, Loss: 0.0403, w: 1.8345, b: 0.4523\n",
      "Epoch 20, Learning Rate: 0.0840, Loss: 0.0215, w: 1.8811, b: 0.3497\n",
      "Epoch 30, Learning Rate: 0.0775, Loss: 0.0131, w: 1.9068, b: 0.2741\n",
      "Epoch 40, Learning Rate: 0.0719, Loss: 0.0084, w: 1.9256, b: 0.2188\n",
      "Epoch 50, Learning Rate: 0.0671, Loss: 0.0055, w: 1.9396, b: 0.1775\n",
      "Epoch 60, Learning Rate: 0.0629, Loss: 0.0037, w: 1.9504, b: 0.1459\n",
      "Epoch 70, Learning Rate: 0.0592, Loss: 0.0026, w: 1.9587, b: 0.1214\n",
      "Epoch 80, Learning Rate: 0.0559, Loss: 0.0018, w: 1.9653, b: 0.1021\n",
      "Epoch 90, Learning Rate: 0.0529, Loss: 0.0013, w: 1.9705, b: 0.0867\n",
      "Epoch 100, Learning Rate: 0.0503, Loss: 0.0009, w: 1.9747, b: 0.0742\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Data\n",
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "y = torch.tensor([2.0, 4.0, 6.0, 8.0])\n",
    "\n",
    "# Parameters\n",
    "w = torch.tensor(0.0, requires_grad=True)\n",
    "b = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "# Initial learning rate and decay rate\n",
    "eta_0 = 0.1\n",
    "k = 0.01\n",
    "\n",
    "# Number of epochs\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Update learning rate\n",
    "    eta = eta_0 / (1 + k * epoch)\n",
    "    \n",
    "    # Forward pass\n",
    "    y_pred = w * x + b\n",
    "    loss = ((y_pred - y) ** 2).mean()\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update parameters\n",
    "    with torch.no_grad():\n",
    "        w -= eta * w.grad\n",
    "        b -= eta * b.grad\n",
    "    \n",
    "    # Zero gradients\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "    \n",
    "    # Print progress every 100 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Learning Rate: {eta:.4f}, Loss: {loss.item():.4f}, w: {w.item():.4f}, b: {b.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nesterov Momentum\n",
    "\n",
    "Momentum methods accelerate gradient descent by moving in the direction of the accumulated gradients (momentum). Nesterov Momentum is a variant that looks ahead by calculating the gradient at the approximate future position.\n",
    "\n",
    "\n",
    "- **Velocity Update:**\n",
    "\n",
    "  $$\n",
    "  v_t = \\gamma v_{t-1} + \\eta \\nabla_{\\theta} f(\\theta_t - \\gamma v_{t-1})\n",
    "  $$\n",
    "\n",
    "- **Parameter Update:**\n",
    "\n",
    "  $$\n",
    "  \\theta_{t+1} = \\theta_t - v_t\n",
    "  $$\n",
    "\n",
    "  - $ \\gamma $: Momentum coefficient (e.g., 0.9)\n",
    "  - $ v_t $: Velocity at time $ t $\n",
    "\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- **Lookahead Parameters:** We compute the gradient at the approximate future position.\n",
    "- **Velocity Update:** Combines momentum and gradient information.\n",
    "- **Parameter Update:** Parameters are updated using the new velocities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.9508, w: 2.0767, b: 0.6248\n",
      "Epoch 20, Loss: 0.0479, w: 1.8983, b: 0.4240\n",
      "Epoch 30, Loss: 0.0218, w: 1.8772, b: 0.2832\n",
      "Epoch 40, Loss: 0.0066, w: 1.9369, b: 0.1972\n",
      "Epoch 50, Loss: 0.0027, w: 1.9573, b: 0.1270\n",
      "Epoch 60, Loss: 0.0011, w: 1.9726, b: 0.0795\n",
      "Epoch 70, Loss: 0.0004, w: 1.9835, b: 0.0487\n",
      "Epoch 80, Loss: 0.0001, w: 1.9901, b: 0.0291\n",
      "Epoch 90, Loss: 0.0000, w: 1.9942, b: 0.0170\n",
      "Epoch 100, Loss: 0.0000, w: 1.9967, b: 0.0097\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Data\n",
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "y = torch.tensor([2.0, 4.0, 6.0, 8.0])\n",
    "\n",
    "# Parameters\n",
    "w = torch.tensor(0.0, requires_grad=True)\n",
    "b = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "# Hyperparameters\n",
    "eta = 0.01\n",
    "gamma = 0.9\n",
    "\n",
    "# Initialize velocities\n",
    "v_w = 0.0\n",
    "v_b = 0.0\n",
    "\n",
    "# Number of epochs\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Lookahead parameters\n",
    "    w_lookahead = w - gamma * v_w\n",
    "    b_lookahead = b - gamma * v_b\n",
    "\n",
    "    # Forward pass with lookahead\n",
    "    y_pred = w_lookahead * x + b_lookahead\n",
    "    loss = ((y_pred - y) ** 2).mean()\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update velocities\n",
    "    v_w = gamma * v_w + eta * w.grad\n",
    "    v_b = gamma * v_b + eta * b.grad\n",
    "\n",
    "    # Update parameters\n",
    "    with torch.no_grad():\n",
    "        w -= v_w\n",
    "        b -= v_b\n",
    "\n",
    "    # Zero gradients\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "\n",
    "    # Print progress every 100 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}, w: {w.item():.4f}, b: {b.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad\n",
    "\n",
    "AdaGrad adapts the learning rate for each parameter individually based on the historical gradients. Parameters with large gradients have their learning rates reduced, while those with small gradients have their learning rates increased.\n",
    "\n",
    "- **Accumulator Update:**\n",
    "\n",
    "  $$\n",
    "  s_t = s_{t-1} + (\\nabla_{\\theta} f(\\theta_t))^2\n",
    "  $$\n",
    "\n",
    "- **Parameter Update:**\n",
    "\n",
    "  $$\n",
    "  \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{s_t + \\epsilon}} \\nabla_{\\theta} f(\\theta_t)\n",
    "  $$\n",
    "\n",
    "  - $ s_t $: Sum of squares of gradients\n",
    "  - $ \\epsilon $: Small constant to prevent division by zero (e.g., $ 1e^{-8} $)\n",
    "\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- **Accumulators $ s_w $ and $ s_b $:** Keep track of the sum of squared gradients.\n",
    "- **Adaptive Learning Rate:** Adjusted for each parameter based on its historical gradients.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.2610, w: 1.5741, b: 1.2138\n",
      "Epoch 20, Loss: 0.1626, w: 1.6639, b: 0.9581\n",
      "Epoch 30, Loss: 0.1014, w: 1.7346, b: 0.7565\n",
      "Epoch 40, Loss: 0.0632, w: 1.7904, b: 0.5975\n",
      "Epoch 50, Loss: 0.0395, w: 1.8344, b: 0.4721\n",
      "Epoch 60, Loss: 0.0246, w: 1.8692, b: 0.3730\n",
      "Epoch 70, Loss: 0.0154, w: 1.8966, b: 0.2947\n",
      "Epoch 80, Loss: 0.0096, w: 1.9183, b: 0.2329\n",
      "Epoch 90, Loss: 0.0060, w: 1.9355, b: 0.1840\n",
      "Epoch 100, Loss: 0.0037, w: 1.9490, b: 0.1454\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Data\n",
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "y = torch.tensor([2.0, 4.0, 6.0, 8.0])\n",
    "\n",
    "# Parameters\n",
    "w = torch.tensor(0.0, requires_grad=True)\n",
    "b = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "# Hyperparameters\n",
    "eta = 1.0  # Higher initial learning rate\n",
    "epsilon = 1e-8\n",
    "\n",
    "# Accumulators\n",
    "s_w = 0.0\n",
    "s_b = 0.0\n",
    "\n",
    "# Number of epochs\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    y_pred = w * x + b\n",
    "    loss = ((y_pred - y) ** 2).mean()\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update accumulators\n",
    "    s_w += w.grad.data ** 2\n",
    "    s_b += b.grad.data ** 2\n",
    "    \n",
    "    # Update parameters\n",
    "    with torch.no_grad():\n",
    "        w -= (eta / (torch.sqrt(s_w) + epsilon)) * w.grad\n",
    "        b -= (eta / (torch.sqrt(s_b) + epsilon)) * b.grad\n",
    "    \n",
    "    # Zero gradients\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "    \n",
    "    # Print progress every 100 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}, w: {w.item():.4f}, b: {b.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaDelta\n",
    "\n",
    "\n",
    "AdaDelta addresses the diminishing learning rates in AdaGrad by restricting the window of accumulated past gradients to some fixed size.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "- **Accumulator Update:**\n",
    "\n",
    "  $$\n",
    "  E[g^2]_t = \\rho E[g^2]_{t-1} + (1 - \\rho) (\\nabla_{\\theta} f(\\theta_t))^2\n",
    "  $$\n",
    "\n",
    "- **Parameter Update:**\n",
    "\n",
    "  $$\n",
    "  \\Delta \\theta_t = - \\frac{\\sqrt{E[\\Delta \\theta^2]_{t-1} + \\epsilon}}{\\sqrt{E[g^2]_t + \\epsilon}} \\nabla_{\\theta} f(\\theta_t)\n",
    "  $$\n",
    "\n",
    "- **Accumulator for Parameter Updates:**\n",
    "\n",
    "  $$\n",
    "  E[\\Delta \\theta^2]_t = \\rho E[\\Delta \\theta^2]_{t-1} + (1 - \\rho) (\\Delta \\theta_t)^2\n",
    "  $$\n",
    "\n",
    "  - $ \\rho $: Decay rate (e.g., 0.9)\n",
    "\n",
    "**PyTorch Implementation:**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# Data\n",
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "y = torch.tensor([2.0, 4.0, 6.0, 8.0])\n",
    "\n",
    "# Parameters\n",
    "w = torch.tensor(0.0, requires_grad=True)\n",
    "b = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "# Hyperparameters\n",
    "rho = 0.9\n",
    "epsilon = 1e-6\n",
    "\n",
    "# Accumulators\n",
    "E_w_grad = 0.0\n",
    "E_b_grad = 0.0\n",
    "E_w_delta = 0.0\n",
    "E_b_delta = 0.0\n",
    "\n",
    "# Number of epochs\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    y_pred = w * x + b\n",
    "    loss = ((y_pred - y) ** 2).mean()\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Accumulate gradients\n",
    "    E_w_grad = rho * E_w_grad + (1 - rho) * w.grad.data ** 2\n",
    "    E_b_grad = rho * E_b_grad + (1 - rho) * b.grad.data ** 2\n",
    "    \n",
    "    # Compute update\n",
    "    delta_w = - (torch.sqrt(E_w_delta + epsilon) / torch.sqrt(E_w_grad + epsilon)) * w.grad.data\n",
    "    delta_b = - (torch.sqrt(E_b_delta + epsilon) / torch.sqrt(E_b_grad + epsilon)) * b.grad.data\n",
    "    \n",
    "    # Accumulate updates\n",
    "    E_w_delta = rho * E_w_delta + (1 - rho) * delta_w ** 2\n",
    "    E_b_delta = rho * E_b_delta + (1 - rho) * delta_b ** 2\n",
    "    \n",
    "    # Update parameters\n",
    "    with torch.no_grad():\n",
    "        w += delta_w\n",
    "        b += delta_b\n",
    "    \n",
    "    # Zero gradients\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "    \n",
    "    # Print progress every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}, w: {w.item():.4f}, b: {b.item():.4f}')\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- **Exponential Moving Averages:** Both the squared gradients and squared parameter updates are tracked.\n",
    "- **Adaptive Updates:** The parameter updates are scaled based on the ratio of accumulated updates to accumulated gradients.\n",
    "\n",
    "---\n",
    "\n",
    "### RMSprop\n",
    "\n",
    "RMSprop is similar to AdaDelta and aims to resolve AdaGrad's diminishing learning rates by using a moving average of squared gradients.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "- **Accumulator Update:**\n",
    "\n",
    "  $$\n",
    "  E[g^2]_t = \\rho E[g^2]_{t-1} + (1 - \\rho) (\\nabla_{\\theta} f(\\theta_t))^2\n",
    "  $$\n",
    "\n",
    "- **Parameter Update:**\n",
    "\n",
    "  $$\n",
    "  \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}} \\nabla_{\\theta} f(\\theta_t)\n",
    "  $$\n",
    "\n",
    "**PyTorch Implementation:**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# Data\n",
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "y = torch.tensor([2.0, 4.0, 6.0, 8.0])\n",
    "\n",
    "# Parameters\n",
    "w = torch.tensor(0.0, requires_grad=True)\n",
    "b = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "# Hyperparameters\n",
    "eta = 0.01\n",
    "rho = 0.9\n",
    "epsilon = 1e-6\n",
    "\n",
    "# Accumulators\n",
    "E_w_grad = 0.0\n",
    "E_b_grad = 0.0\n",
    "\n",
    "# Number of epochs\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    y_pred = w * x + b\n",
    "    loss = ((y_pred - y) ** 2).mean()\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Accumulate squared gradients\n",
    "    E_w_grad = rho * E_w_grad + (1 - rho) * w.grad.data ** 2\n",
    "    E_b_grad = rho * E_b_grad + (1 - rho) * b.grad.data ** 2\n",
    "    \n",
    "    # Update parameters\n",
    "    with torch.no_grad():\n",
    "        w -= (eta / (torch.sqrt(E_w_grad + epsilon))) * w.grad\n",
    "        b -= (eta / (torch.sqrt(E_b_grad + epsilon))) * b.grad\n",
    "    \n",
    "    # Zero gradients\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "    \n",
    "    # Print progress every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}, w: {w.item():.4f}, b: {b.item():.4f}')\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- **Exponential Moving Average of Squared Gradients:** Helps in adapting the learning rate.\n",
    "- **Parameter Update:** Similar to AdaGrad but with a moving average to prevent the learning rate from becoming too small.\n",
    "\n",
    "---\n",
    "\n",
    "###  Adam (Adaptive Moment Estimation)\n",
    "\n",
    "Adam combines ideas from both Momentum and RMSprop. It keeps an exponentially decaying average of past gradients (momentum) and squared gradients (adaptive learning rates).\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "- **First Moment Estimate (Mean):**\n",
    "\n",
    "  $$\n",
    "  m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) \\nabla_{\\theta} f(\\theta_t)\n",
    "  $$\n",
    "\n",
    "- **Second Moment Estimate (Variance):**\n",
    "\n",
    "  $$\n",
    "  v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) (\\nabla_{\\theta} f(\\theta_t))^2\n",
    "  $$\n",
    "\n",
    "- **Bias Correction:**\n",
    "\n",
    "  $$\n",
    "  \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}\n",
    "  $$\n",
    "  \n",
    "  $$\n",
    "  \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n",
    "  $$\n",
    "\n",
    "- **Parameter Update:**\n",
    "\n",
    "  $$\n",
    "  \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t\n",
    "  $$\n",
    "\n",
    "  - $ \\beta_1 $: Decay rate for the first moment (e.g., 0.9)\n",
    "  - $ \\beta_2 $: Decay rate for the second moment (e.g., 0.999)\n",
    "  - $ \\epsilon $: Small constant to prevent division by zero (e.g., $ 1e^{-8} $)\n",
    "\n",
    "**PyTorch Implementation:**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# Data\n",
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "y = torch.tensor([2.0, 4.0, 6.0, 8.0])\n",
    "\n",
    "# Parameters\n",
    "w = torch.tensor(0.0, requires_grad=True)\n",
    "b = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "# Hyperparameters\n",
    "eta = 0.1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "epsilon = 1e-8\n",
    "\n",
    "# Initialize moment estimates\n",
    "m_w = 0.0\n",
    "v_w = 0.0\n",
    "m_b = 0.0\n",
    "v_b = 0.0\n",
    "\n",
    "# Time step\n",
    "t = 0\n",
    "\n",
    "# Number of epochs\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    t += 1\n",
    "\n",
    "    # Forward pass\n",
    "    y_pred = w * x + b\n",
    "    loss = ((y_pred - y) ** 2).mean()\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update biased first moment estimate\n",
    "    m_w = beta1 * m_w + (1 - beta1) * w.grad.data\n",
    "    m_b = beta1 * m_b + (1 - beta1) * b.grad.data\n",
    "\n",
    "    # Update biased second raw moment estimate\n",
    "    v_w = beta2 * v_w + (1 - beta2) * (w.grad.data ** 2)\n",
    "    v_b = beta2 * v_b + (1 - beta2) * (b.grad.data ** 2)\n",
    "\n",
    "    # Compute bias-corrected first moment estimate\n",
    "    m_w_hat = m_w / (1 - beta1 ** t)\n",
    "    m_b_hat = m_b / (1 - beta1 ** t)\n",
    "\n",
    "    # Compute bias-corrected second raw moment estimate\n",
    "    v_w_hat = v_w / (1 - beta2 ** t)\n",
    "    v_b_hat = v_b / (1 - beta2 ** t)\n",
    "\n",
    "    # Update parameters\n",
    "    with torch.no_grad():\n",
    "        w -= (eta / (torch.sqrt(v_w_hat) + epsilon)) * m_w_hat\n",
    "        b -= (eta / (torch.sqrt(v_b_hat) + epsilon)) * m_b_hat\n",
    "\n",
    "    # Zero gradients\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "\n",
    "    # Print progress every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item():.6f}, w: {w.item():.4f}, b: {b.item():.4f}')\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- **First Moment (Mean):** Tracks the average of the gradients (momentum).\n",
    "- **Second Moment (Variance):** Tracks the average of the squared gradients (adaptive learning rate).\n",
    "- **Bias Correction:** Adjusts the estimates to account for their initialization at zero.\n",
    "- **Parameter Update:** Combines both momentum and adaptive learning rate for efficient optimization.\n",
    "\n",
    "---\n",
    "\n",
    "**Understanding the Need for Advanced Methods**\n",
    "\n",
    "- **Adaptive Learning Rates:** Methods like AdaGrad, AdaDelta, RMSprop, and Adam adjust the learning rate for each parameter based on the historical gradients. This is crucial when dealing with sparse data or when different parameters require different learning rates.\n",
    "\n",
    "- **Momentum:** Nesterov Momentum and Adam incorporate momentum to accelerate convergence, especially in regions with small but consistent gradients.\n",
    "\n",
    "- **Avoiding Diminishing Learning Rates:** AdaDelta and RMSprop address the problem of AdaGrad's learning rate shrinking too much, ensuring that learning continues throughout training.\n",
    "\n",
    "**Final Thoughts**\n",
    "\n",
    "Advanced gradient descent methods are essential for training deep neural networks efficiently. They help in faster convergence, escaping local minima, and adapting to the complexities of different datasets. Understanding these algorithms enables practitioners to choose the right optimizer for their specific problem.\n",
    "\n",
    "Feel free to experiment with the provided PyTorch code to observe how each optimizer affects the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "Regularization techniques are crucial in training neural networks as they help prevent overfitting, improve generalization, and stabilize the learning process. Overfitting occurs when a model learns the training data too well, including its noise, leading to poor performance on unseen data. Regularization introduces additional information or constraints to encourage simpler models that generalize better.\n",
    "\n",
    "we'll delve into three common regularization methods:\n",
    "\n",
    "1. **L1 and L2 Regularization**\n",
    "2. **Dropout**\n",
    "3. **Batch Normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 and L2 Regularization\n",
    "\n",
    "#### L2 Regularization (Weight Decay)\n",
    "\n",
    "- Objective: Penalize large weights to encourage the network to keep weights small.\n",
    "- Modified Loss Function:\n",
    "\n",
    "$$\n",
    "  L_{\\text{new}} = L_{\\text{original}} + \\lambda \\sum_{i} w_{i}^{2}\n",
    "$$\n",
    "\n",
    "  where:\n",
    "  - $ L_{\\text{original}} $ is the original loss (e.g., Mean Squared Error).\n",
    "  - $ \\lambda $ is the regularization parameter controlling the penalty strength.\n",
    "  - $ w_{i} $ are the model weights.\n",
    "\n",
    "- **Gradient Update:**\n",
    "\n",
    "  The gradient of the regularized loss w.r.t weights includes an additional term:\n",
    "\n",
    "$$\n",
    "  \\frac{\\partial L_{\\text{new}}}{\\partial w_{i}} = \\frac{\\partial L_{\\text{original}}}{\\partial w_{i}} + 2\\lambda w_{i}\n",
    " $$\n",
    "\n",
    "  This leads to a \"weight decay\" effect during optimization.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Consider a linear regression model:\n",
    "\n",
    "$$\n",
    "y = w_{1} x_{1} + w_{2} x_{2} + b\n",
    "$$\n",
    "\n",
    "With L2 regularization, the model aims to minimize:\n",
    "\n",
    "$$\n",
    "L_{\\text{new}} = (y_{\\text{true}} - y_{\\text{pred}})^{2} + \\lambda (w_{1}^{2} + w_{2}^{2})\n",
    "$$\n",
    "\n",
    "This penalizes large weights, leading to a smoother model.\n",
    "\n",
    "#### **L1 Regularization**\n",
    "\n",
    "**Mathematical Explanation:**\n",
    "\n",
    "- **Objective:** Promote sparsity in weights, effectively performing feature selection.\n",
    "- **Modified Loss Function:**\n",
    "\n",
    "$$\n",
    "  L_{\\text{new}} = L_{\\text{original}} + \\lambda \\sum_{i} |w_{i}|\n",
    " $$\n",
    "\n",
    "- **Gradient Update:**\n",
    "\n",
    "$$\n",
    "  \\frac{\\partial L_{\\text{new}}}{\\partial w_{i}} = \\frac{\\partial L_{\\text{original}}}{\\partial w_{i}} + \\lambda \\cdot \\text{sign}(w_{i})\n",
    " $$\n",
    "\n",
    "  This encourages weights to become exactly zero.\n",
    "\n",
    "**Toy Example:**\n",
    "\n",
    "Using the same linear regression model, L1 regularization modifies the loss to:\n",
    "\n",
    "$$\n",
    "L_{\\text{new}} = (y_{\\text{true}} - y_{\\text{pred}})^{2} + \\lambda (|w_{1}| + |w_{2}|)\n",
    "$$\n",
    "\n",
    "Weights that contribute less to minimizing the loss are driven to zero, simplifying the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Dropout\n",
    "\n",
    "**Mathematical Explanation:**\n",
    "\n",
    "- **Objective:** Prevent co-adaptation of neurons by randomly dropping units during training.\n",
    "- **Mechanism:**\n",
    "\n",
    "  - During each training iteration, each neuron (along with its connections) is kept with probability $ p $ or dropped with probability $ 1 - p $.\n",
    "  - Effectively trains an ensemble of subnetworks and averages their predictions.\n",
    "\n",
    "**Toy Example:**\n",
    "\n",
    "Consider a neural network layer with neurons $ n_{1}, n_{2}, n_{3} $. During one iteration:\n",
    "\n",
    "- Neuron $ n_{2} $ is dropped (set to zero).\n",
    "- The network computes output using only $ n_{1} $ and $ n_{3} $.\n",
    "- In the next iteration, different neurons may be dropped.\n",
    "\n",
    "This randomness forces the network to learn redundant representations, enhancing robustness.\n",
    "\n",
    "\n",
    "###  Batch Normalization\n",
    "\n",
    "**Mathematical Explanation:**\n",
    "\n",
    "- **Objective:** Normalize inputs of each layer to stabilize and accelerate training.\n",
    "- **Process:**\n",
    "\n",
    "  For a mini-batch $ B = \\{ x_{1}, x_{2}, ..., x_{m} \\} $:\n",
    "\n",
    "  1. **Compute Mean and Variance:**\n",
    "\n",
    "   $$\n",
    "     \\mu_{B} = \\frac{1}{m} \\sum_{i=1}^{m} x_{i} \\\\\n",
    "     \\sigma_{B}^{2} = \\frac{1}{m} \\sum_{i=1}^{m} (x_{i} - \\mu_{B})^{2}\n",
    "    $$\n",
    "\n",
    "  2. **Normalize:**\n",
    "\n",
    "   $$\n",
    "     \\hat{x}_{i} = \\frac{x_{i} - \\mu_{B}}{\\sqrt{\\sigma_{B}^{2} + \\epsilon}}\n",
    "    $$\n",
    "\n",
    "     where $ \\epsilon $ is a small constant to prevent division by zero.\n",
    "\n",
    "  3. **Scale and Shift:**\n",
    "\n",
    "   $$\n",
    "     y_{i} = \\gamma \\hat{x}_{i} + \\beta\n",
    "    $$\n",
    "\n",
    "     $ \\gamma $ and $ \\beta $ are learnable parameters.\n",
    "\n",
    "- **Benefits:**\n",
    "\n",
    "  - Reduces internal covariate shift.\n",
    "  - Allows for higher learning rates.\n",
    "  - Acts as a form of regularization due to mini-batch noise.\n",
    "\n",
    "**Toy Example:**\n",
    "\n",
    "In training, inputs to a layer vary due to parameter updates in previous layers. Batch normalization standardizes these inputs, making the optimization landscape smoother and improving convergence.\n",
    "\n",
    "---\n",
    "\n",
    "### **Implementing Regularization Techniques in PyTorch**\n",
    "\n",
    "We'll create a simple neural network for a classification task using the MNIST dataset and apply each regularization method.\n",
    "\n",
    "#### **Setup**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "```\n",
    "\n",
    "#### **Data Loading**\n",
    "\n",
    "```python\n",
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Datasets\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Data Loaders\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader  = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1000, shuffle=False)\n",
    "```\n",
    "\n",
    "#### **Model Definition**\n",
    "\n",
    "We'll define a simple feedforward neural network and demonstrate how to incorporate each regularization technique.\n",
    "\n",
    "##### **Without Regularization**\n",
    "\n",
    "```python\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 256)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "model = SimpleNN()\n",
    "```\n",
    "\n",
    "#### **1. Adding L2 Regularization**\n",
    "\n",
    "PyTorch optimizers have a `weight_decay` parameter that applies L2 regularization.\n",
    "\n",
    "```python\n",
    "# L2 Regularization with weight_decay\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "```\n",
    "\n",
    "**Training Loop with L2 Regularization**\n",
    "\n",
    "```python\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(10):\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "```\n",
    "\n",
    "#### **2. Adding L1 Regularization**\n",
    "\n",
    "PyTorch does not have built-in support for L1 regularization, so we manually add the L1 penalty to the loss.\n",
    "\n",
    "```python\n",
    "# L1 Regularization parameter\n",
    "l1_lambda = 1e-5\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # L1 Regularization\n",
    "        l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "        loss = loss + l1_lambda * l1_norm\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "```\n",
    "\n",
    "#### **3. Adding Dropout**\n",
    "\n",
    "We modify the model to include `nn.Dropout` layers.\n",
    "\n",
    "```python\n",
    "class DropoutNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DropoutNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 256)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "model = DropoutNN()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "```\n",
    "\n",
    "**Training Loop with Dropout**\n",
    "\n",
    "Same as before, but now the model includes dropout layers.\n",
    "\n",
    "#### **4. Adding Batch Normalization**\n",
    "\n",
    "We insert `nn.BatchNorm1d` layers after linear layers.\n",
    "\n",
    "```python\n",
    "class BatchNormNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BatchNormNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        out = self.fc1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "model = BatchNormNN()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "```\n",
    "\n",
    "**Training Loop with Batch Normalization**\n",
    "\n",
    "Same as before. Batch normalization layers are updated automatically during training.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "- **L1/L2 Regularization:** Penalize large weights to simplify the model and prevent overfitting.\n",
    "- **Dropout:** Randomly drops neurons during training to prevent co-adaptation and improve generalization.\n",
    "- **Batch Normalization:** Normalizes layer inputs to stabilize training, allowing for higher learning rates and acting as regularization.\n",
    "\n",
    "Implementing these techniques in PyTorch involves modifying the loss function or model architecture:\n",
    "\n",
    "- **L2 Regularization:** Use `weight_decay` in the optimizer.\n",
    "- **L1 Regularization:** Manually add L1 penalty to the loss.\n",
    "- **Dropout:** Add `nn.Dropout` layers to the model.\n",
    "- **Batch Normalization:** Add `nn.BatchNorm1d` or `nn.BatchNorm2d` layers after linear or convolutional layers.\n",
    "\n",
    "These regularization methods are essential tools in a deep learning practitioner's toolkit, helping to build models that perform well on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_notes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
