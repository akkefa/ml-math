{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Optimization\n",
    "\n",
    "## Gradient Descent Algorithm\n",
    "\n",
    "\n",
    "Gradient Descent is an optimization algorithm used to minimize a function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. It's widely used in machine learning to update parameters of models.\n",
    "\n",
    "Mathematical Explanation:\n",
    "\n",
    "Given a function $ f(\\theta) $ where $ \\theta $ represents the parameters, the goal is to find $ \\theta $ that minimizes $ f(\\theta) $.\n",
    "\n",
    "Update Rule:\n",
    "\n",
    "$$\n",
    "\\theta_{\\text{new}} = \\theta_{\\text{old}} - \\eta \\nabla_{\\theta} f(\\theta_{\\text{old}})\n",
    "$$\n",
    "\n",
    "- $ \\eta $ is the learning rate (a small positive number).\n",
    "- $ \\nabla_{\\theta} f(\\theta_{\\text{old}}) $ is the gradient of the function at $ \\theta_{\\text{old}} $.\n",
    "\n",
    "Visual Illustration:\n",
    "\n",
    "Imagine you're at the top of a hill (the maximum of the function), and you want to get to the bottom (the minimum). At each step, you look around for the steepest downward slope (the negative gradient) and take a step in that direction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Stochastic Gradient Descent is a variation of gradient descent where the gradient is estimated using a single sample (or a mini-batch) rather than the entire dataset. This makes it computationally efficient and allows it to handle large datasets.\n",
    "\n",
    "Update Rule:\n",
    "\n",
    "$$\n",
    "\\theta_{\\text{new}} = \\theta_{\\text{old}} - \\eta \\nabla_{\\theta} f(\\theta_{\\text{old}}; x_i, y_i)\n",
    "$$\n",
    "\n",
    "- $ (x_i, y_i) $ is a single data point.\n",
    "- The gradient $ \\nabla_{\\theta} f(\\theta_{\\text{old}}; x_i, y_i) $ is computed using only this data point.\n",
    "\n",
    "Benefits of SGD:\n",
    "\n",
    "- Faster iterations due to less computation per update.\n",
    "- Introduces noise that can help escape local minima.\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Linear Regression with Gradient Descent\n",
    "\n",
    "Suppose we have a dataset:\n",
    "\n",
    "| $ x_i $ | $ y_i $ |\n",
    "|-----------|-----------|\n",
    "|     1     |     2     |\n",
    "|     2     |     4     |\n",
    "|     3     |     6     |\n",
    "|     4     |     8     |\n",
    "\n",
    "We want to fit a linear model $ y = w x + b $ using gradient descent.\n",
    "\n",
    "Loss Function (Mean Squared Error):\n",
    "\n",
    "$$\n",
    "L(w, b) = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - (w x_i + b))^2\n",
    "$$\n",
    "\n",
    "Compute Gradients:\n",
    "\n",
    "- Gradient with respect to $ w $:\n",
    "\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial w} = -\\frac{2}{N} \\sum_{i=1}^{N} x_i (y_i - (w x_i + b))\n",
    "  $$\n",
    "\n",
    "- Gradient with respect to $ b $:\n",
    "\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial b} = -\\frac{2}{N} \\sum_{i=1}^{N} (y_i - (w x_i + b))\n",
    "  $$\n",
    "\n",
    "Update Rules:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "w_{\\text{new}} & = w_{\\text{old}} - \\eta \\frac{\\partial L}{\\partial w} \\\\\n",
    "b_{\\text{new}} & = b_{\\text{old}} - \\eta \\frac{\\partial L}{\\partial b}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Step-by-Step Calculation:\n",
    "\n",
    "Let's initialize $ w = 0 $, $ b = 0 $, and $ \\eta = 0.01 $.\n",
    "\n",
    "First Iteration:\n",
    "\n",
    "1. Compute predictions:\n",
    "\n",
    "   $$\n",
    "   \\hat{y}_i = w x_i + b = 0 \\times x_i + 0 = 0\n",
    "   $$\n",
    "\n",
    "2. Compute gradients:\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial w} = -\\frac{2}{4} \\sum_{i=1}^{4} x_i (y_i - \\hat{y}_i) = -\\frac{1}{2} \\sum_{i=1}^{4} x_i y_i\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial b} = -\\frac{2}{4} \\sum_{i=1}^{4} (y_i - \\hat{y}_i) = -\\frac{1}{2} \\sum_{i=1}^{4} y_i\n",
    "   $$\n",
    "\n",
    "3. Calculate sums:\n",
    "\n",
    "   $$\n",
    "   \\sum_{i=1}^{4} x_i y_i = 1 \\times 2 + 2 \\times 4 + 3 \\times 6 + 4 \\times 8 = 60\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   \\sum_{i=1}^{4} y_i = 2 + 4 + 6 + 8 = 20\n",
    "   $$\n",
    "\n",
    "4. Compute gradients:\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial w} = -\\frac{1}{2} \\times 60 = -30\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial b} = -\\frac{1}{2} \\times 20 = -10\n",
    "   $$\n",
    "\n",
    "5. Update parameters:\n",
    "\n",
    "   $$\n",
    "   w_{\\text{new}} = 0 - 0.01 \\times (-30) = 0 + 0.3 = 0.3\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   b_{\\text{new}} = 0 - 0.01 \\times (-10) = 0 + 0.1 = 0.1\n",
    "   $$\n",
    "\n",
    "Final Thoughts:\n",
    "\n",
    "- Gradient Descent is suitable for smaller datasets where computational efficiency is not a concern.\n",
    "- Stochastic Gradient Descent is better for larger datasets and can help in escaping local minima due to its stochastic nature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: w = 1.5104, b = 0.4936, Loss = 1.1751\n",
      "Epoch 20: w = 1.7583, b = 0.5584, Loss = 0.0843\n",
      "Epoch 30: w = 1.8030, b = 0.5547, Loss = 0.0529\n",
      "Epoch 40: w = 1.8149, b = 0.5404, Loss = 0.0492\n",
      "Epoch 50: w = 1.8213, b = 0.5248, Loss = 0.0463\n",
      "Epoch 60: w = 1.8267, b = 0.5093, Loss = 0.0436\n",
      "Epoch 70: w = 1.8319, b = 0.4943, Loss = 0.0410\n",
      "Epoch 80: w = 1.8369, b = 0.4797, Loss = 0.0387\n",
      "Epoch 90: w = 1.8417, b = 0.4655, Loss = 0.0364\n",
      "Epoch 100: w = 1.8463, b = 0.4518, Loss = 0.0343\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Data\n",
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "y = torch.tensor([2.0, 4.0, 6.0, 8.0])\n",
    "\n",
    "# Parameters\n",
    "w = torch.tensor(0.0, requires_grad=True)\n",
    "b = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "# Learning rate\n",
    "eta = 0.01\n",
    "\n",
    "# Number of epochs\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = w * x + b\n",
    "    # Compute and print loss\n",
    "    loss = ((y_pred - y) ** 2).mean()\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    # Update parameters\n",
    "    with torch.no_grad():\n",
    "        w -= eta * w.grad\n",
    "        b -= eta * b.grad\n",
    "    # Zero gradients\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "    # Print progress\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}: w = {w.item():.4f}, b = {b.item():.4f}, Loss = {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: w = 1.9437, b = 0.1718, Loss = 0.0049\n",
      "Epoch 20: w = 1.9503, b = 0.1516, Loss = 0.0038\n",
      "Epoch 30: w = 1.9554, b = 0.1333, Loss = 0.0030\n",
      "Epoch 40: w = 1.9613, b = 0.1178, Loss = 0.0023\n",
      "Epoch 50: w = 1.9648, b = 0.1036, Loss = 0.0018\n",
      "Epoch 60: w = 1.9693, b = 0.0914, Loss = 0.0014\n",
      "Epoch 70: w = 1.9733, b = 0.0807, Loss = 0.0011\n",
      "Epoch 80: w = 1.9762, b = 0.0711, Loss = 0.0008\n",
      "Epoch 90: w = 1.9792, b = 0.0628, Loss = 0.0007\n",
      "Epoch 100: w = 1.9809, b = 0.0551, Loss = 0.0005\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "# Data\n",
    "x = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n",
    "y = torch.tensor([[2.0], [4.0], [6.0], [8.0]])\n",
    "\n",
    "# Parameters\n",
    "w = torch.randn(1, 1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "\n",
    "# Learning rate\n",
    "eta = 0.01\n",
    "\n",
    "# Number of epochs\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    permutation = torch.randperm(x.size()[0])\n",
    "    for i in permutation:\n",
    "        xi = x[i]\n",
    "        yi = y[i]\n",
    "        # Forward pass\n",
    "        y_pred = xi @ w + b\n",
    "        # Compute loss\n",
    "        loss = (y_pred - yi).pow(2).mean()\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Update parameters\n",
    "        with torch.no_grad():\n",
    "            w -= eta * w.grad\n",
    "            b -= eta * b.grad\n",
    "        # Zero gradients\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "    # Print progress\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        # Compute total loss\n",
    "        y_pred = x @ w + b\n",
    "        loss = (y_pred - y).pow(2).mean()\n",
    "        print(f'Epoch {epoch+1}: w = {w.item():.4f}, b = {b.item():.4f}, Loss = {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_notes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
