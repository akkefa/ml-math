{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization\n",
    "\n",
    "Matrix factorization is a class of collaborative filtering algorithms used in recommender systems. Matrix factorization algorithms work by decomposing the user-item interaction matrix into the product of two lower dimensionality rectangular matrices. The rows of the first matrix represent the latent user factors and the columns of the second matrix represent the latent item factors. The dot product of these two matrices approximates the original user-item interaction matrix. The latent factors are also known as embeddings and are typically of much lower dimensionality than the original user and item vectors. The latent factors are learned through an iterative process that minimizes the error between the dot product of the latent factors and the original user-item interaction matrix. The error is measured using a loss function such as mean squared error (MSE) or binary cross entropy (BCE). The loss function is minimized using gradient descent or one of its variants.\n",
    "\n",
    "## Singular Value Decomposition (SVD)\n",
    "\n",
    "So the singular value decomposition comes from linear algebra, and it's a way of breaking down a matrix into constituent parts. we can factorize it into three matrices. And this is called factorization because it works a lot like factoring numbers. You take 15, and you can factorize it into 3 and 5, such that you multiply 3 and 5 together, and you get 15.\n",
    "\n",
    "$$\n",
    "R=P \\Sigma Q^{\\mathrm{T}}\n",
    "$$\n",
    "\n",
    "- $R$ is $m \\times n$ ratings matrix\n",
    "- $P$ is $m \\times k$ user-feature affinity matrix\n",
    "- $Q$ is $n \\times k$ item-feature relevance matrix\n",
    "- $\\Sigma$ is $k \\times k$ diagonal feature weight matrix\n",
    "- For linear algebra people: $P$ and $Q$ are orthogonal\n",
    "- Linear algebra guarantees this exists for any real $R$\n",
    "\n",
    "\n",
    "### latent features\n",
    "\n",
    "Latent means not directly observable. The common use of the term in PCA and Factor Analysis is to reduce dimension of a large number of directly observable features into a smaller set of indirectly observable features.\n",
    "\n",
    "- SVD describes preference in terms of latent features\n",
    "- These features are learned from the rating data\n",
    "- Not necessarily interpretable\n",
    "    - Optimized for predictive power\n",
    "- Defines a shared vector space for users and items (feature space)\n",
    "    - Enables compact representation of each\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
