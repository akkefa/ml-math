{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization\n",
    "\n",
    "Matrix factorization is a mathematical technique used to decompose a matrix into two or more matrices such that when multiplied together, they approximate the original matrix. This technique is widely used in various fields like collaborative filtering in recommendation systems, natural language processing, and dimensionality reduction.\n",
    "\n",
    "## Why Matrix Factorization Was Invented\n",
    "\n",
    "Matrix factorization was invented to solve several key problems:\n",
    "\n",
    "1. **Dimensionality Reduction**: In high-dimensional datasets, matrix factorization helps reduce the number of dimensions while retaining essential information. This is crucial in tasks like image processing and natural language processing.\n",
    "\n",
    "2. **Collaborative Filtering**: In recommendation systems, matrix factorization is used to predict user preferences for items (e.g., movies, products) by uncovering latent factors representing both users and items.\n",
    "\n",
    "3. **Noise Reduction**: By approximating the original matrix, matrix factorization helps in removing noise and capturing underlying patterns in the data.\n",
    "\n",
    "4. **Data Compression**: It allows for the compression of large datasets by representing them with smaller matrices.\n",
    "\n",
    "## Problems Solved by Matrix Factorization\n",
    "\n",
    "1. **Sparse Data**: Many real-world datasets, such as user-item ratings in recommendation systems, are sparse (contain many missing values). Matrix factorization helps in predicting the missing values.\n",
    "\n",
    "2. **Pattern Recognition**: It helps in recognizing patterns and structures in the data that are not immediately obvious.\n",
    "\n",
    "3. **Improved Recommendations**: By capturing latent factors, matrix factorization improves the accuracy of recommendations.\n",
    "\n",
    "## Mathematical Explanation\n",
    "\n",
    "Matrix factorization involves decomposing a matrix $ A $ into two matrices $ W $ and $ H $ such that $ A \\approx WH $. Here is a detailed explanation:\n",
    "\n",
    "Given a matrix $ A $ of size $ m \\times n $, matrix factorization aims to find two matrices $ W $ (of size $ m \\times k $) and $ H $ (of size $ k \\times n $) such that:\n",
    "\n",
    "$$ A \\approx WH $$\n",
    "\n",
    "### Objective Function\n",
    "\n",
    "One common approach is to minimize the Frobenius norm of the difference between $ A $ and $ WH $:\n",
    "\n",
    "$$ \\min_{W, H} \\| A - WH \\|_F^2 $$\n",
    "\n",
    "Where $ \\| \\cdot \\|_F $ denotes the Frobenius norm, which is defined as:\n",
    "\n",
    "$$ \\| A - WH \\|_F^2 = \\sum_{i=1}^m \\sum_{j=1}^n (A_{ij} - (WH)_{ij})^2 $$\n",
    "\n",
    "### Optimization\n",
    "\n",
    "To find $ W $ and $ H $, we typically use optimization techniques like gradient descent. The update rules for gradient descent are:\n",
    "\n",
    "$$ W \\leftarrow W - \\eta \\frac{\\partial}{\\partial W} \\| A - WH \\|_F^2 $$\n",
    "$$ H \\leftarrow H - \\eta \\frac{\\partial}{\\partial H} \\| A - WH \\|_F^2 $$\n",
    "\n",
    "Where $ \\eta $ is the learning rate.\n",
    "\n",
    "## Toy Example\n",
    "\n",
    "Suppose we have the following matrix $ A $:\n",
    "\n",
    "$$ A = \\begin{bmatrix} 5 & 3 \\\\ 3 & 2 \\\\ 2 & 1 \\\\ 1 & 1 \\end{bmatrix} $$\n",
    "\n",
    "We want to factorize this into two matrices $ W $ and $ H $ such that $ A \\approx WH $.\n",
    "\n",
    "### Code in PyTorch\n",
    "\n",
    "Here's a simple implementation of matrix factorization in PyTorch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ikram.ali/miniconda3/envs/ml_notes/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 500, loss: 6.450703949667513e-05\n",
      "Iteration 1000, loss: 8.83535022921933e-09\n",
      "Iteration 1500, loss: 2.0268231537556858e-12\n",
      "Iteration 2000, loss: 9.698908343125368e-13\n",
      "Iteration 2500, loss: 5.773159728050814e-13\n",
      "Iteration 3000, loss: 1.7275070263167436e-13\n",
      "Iteration 3500, loss: 1.567634910770721e-13\n",
      "Iteration 4000, loss: 3.019806626980426e-14\n",
      "Iteration 4500, loss: 2.842170943040401e-14\n",
      "Iteration 5000, loss: 1.7763568394002505e-15\n",
      "Original Matrix A:\n",
      "tensor([[5., 3.],\n",
      "        [3., 2.],\n",
      "        [2., 1.],\n",
      "        [1., 1.]])\n",
      "\n",
      "Factorized Matrices W and H:\n",
      "tensor([[ 2.0369,  2.0575],\n",
      "        [ 1.4867,  0.9807],\n",
      "        [ 0.5502,  1.0768],\n",
      "        [ 0.9365, -0.0961]], requires_grad=True)\n",
      "tensor([[1.1957, 1.1052],\n",
      "        [1.2464, 0.3640]], requires_grad=True)\n",
      "\n",
      "Reconstructed Matrix A from W and H:\n",
      "tensor([[5.0000, 3.0000],\n",
      "        [3.0000, 2.0000],\n",
      "        [2.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Toy example matrix A\n",
    "A = torch.tensor([[5, 3],\n",
    "                  [3, 2],\n",
    "                  [2, 1],\n",
    "                  [1, 1]], dtype=torch.float32)\n",
    "\n",
    "# Dimensions\n",
    "m, n = A.size()\n",
    "k = 2  # Number of latent factors\n",
    "\n",
    "# Initialize W and H with random values\n",
    "W = torch.rand(m, k, requires_grad=True)\n",
    "H = torch.rand(k, n, requires_grad=True)\n",
    "\n",
    "# Define the learning rate and number of iterations\n",
    "learning_rate = 0.01\n",
    "num_iterations = 5000\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam([W, H], lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for iteration in range(num_iterations):\n",
    "    optimizer.zero_grad()\n",
    "    A_pred = torch.matmul(W, H)\n",
    "    loss = torch.nn.functional.mse_loss(A_pred, A)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (iteration+1) % 500 == 0:\n",
    "        print(f\"Iteration {iteration+1}, loss: {loss.item()}\")\n",
    "\n",
    "print(\"Original Matrix A:\")\n",
    "print(A)\n",
    "\n",
    "print(\"\\nFactorized Matrices W and H:\")\n",
    "print(W)\n",
    "print(H)\n",
    "\n",
    "print(\"\\nReconstructed Matrix A from W and H:\")\n",
    "print(torch.matmul(W, H))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of the Code\n",
    "\n",
    "1. **Initialization**: We initialize the matrices $ W $ and $ H $ with random values.\n",
    "2. **Optimization**: We use the Adam optimizer to update the values of $ W $ and $ H $.\n",
    "3. **Training Loop**: In each iteration, we compute the predicted matrix $ A_{pred} $ by multiplying $ W $ and $ H $. We then compute the mean squared error loss between $ A $ and $ A_{pred} $ and perform backpropagation to update $ W $ and $ H $.\n",
    "\n",
    "This implementation iteratively minimizes the reconstruction error, resulting in matrices $ W $ and $ H $ that approximate the original matrix $ A $."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_notes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
