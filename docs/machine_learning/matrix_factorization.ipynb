{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization\n",
    "\n",
    "Matrix factorization is a mathematical technique used to decompose a matrix into two or more matrices such that when multiplied together, they approximate the original matrix. This technique is widely used in various fields like collaborative filtering in recommendation systems, natural language processing, and dimensionality reduction.\n",
    "\n",
    "## Why Matrix Factorization Was Invented\n",
    "\n",
    "Matrix factorization was invented to solve several key problems:\n",
    "\n",
    "1. **Dimensionality Reduction**: In high-dimensional datasets, matrix factorization helps reduce the number of dimensions while retaining essential information. This is crucial in tasks like image processing and natural language processing.\n",
    "\n",
    "2. **Collaborative Filtering**: In recommendation systems, matrix factorization is used to predict user preferences for items (e.g., movies, products) by uncovering latent factors representing both users and items.\n",
    "\n",
    "3. **Noise Reduction**: By approximating the original matrix, matrix factorization helps in removing noise and capturing underlying patterns in the data.\n",
    "\n",
    "4. **Data Compression**: It allows for the compression of large datasets by representing them with smaller matrices.\n",
    "\n",
    "### Problems Solved by Matrix Factorization\n",
    "\n",
    "1. **Sparse Data**: Many real-world datasets, such as user-item ratings in recommendation systems, are sparse (contain many missing values). Matrix factorization helps in predicting the missing values.\n",
    "\n",
    "2. **Pattern Recognition**: It helps in recognizing patterns and structures in the data that are not immediately obvious.\n",
    "\n",
    "3. **Improved Recommendations**: By capturing latent factors, matrix factorization improves the accuracy of recommendations.\n",
    "\n",
    "### Mathematical Explanation\n",
    "\n",
    "Matrix factorization involves decomposing a matrix $ A $ into two matrices $ W $ and $ H $ such that $ A \\approx WH $. Here is a detailed explanation:\n",
    "\n",
    "Given a matrix $ A $ of size $ m \\times n $, matrix factorization aims to find two matrices $ W $ (of size $ m \\times k $) and $ H $ (of size $ k \\times n $) such that:\n",
    "\n",
    "$$ A \\approx WH $$\n",
    "\n",
    "### Objective Function\n",
    "\n",
    "One common approach is to minimize the Frobenius norm of the difference between $ A $ and $ WH $:\n",
    "\n",
    "$$ \\min_{W, H} \\| A - WH \\|_F^2 $$\n",
    "\n",
    "Where $ \\| \\cdot \\|_F $ denotes the Frobenius norm, which is defined as:\n",
    "\n",
    "$$ \\| A - WH \\|_F^2 = \\sum_{i=1}^m \\sum_{j=1}^n (A_{ij} - (WH)_{ij})^2 $$\n",
    "\n",
    "#### Optimization\n",
    "\n",
    "To find $ W $ and $ H $, we typically use optimization techniques like gradient descent. The update rules for gradient descent are:\n",
    "\n",
    "$$ W \\leftarrow W - \\eta \\frac{\\partial}{\\partial W} \\| A - WH \\|_F^2 $$\n",
    "$$ H \\leftarrow H - \\eta \\frac{\\partial}{\\partial H} \\| A - WH \\|_F^2 $$\n",
    "\n",
    "Where $ \\eta $ is the learning rate.\n",
    "\n",
    "### Toy Example\n",
    "\n",
    "Suppose we have the following matrix $ A $:\n",
    "\n",
    "$$ A = \\begin{bmatrix} 5 & 3 \\\\ 3 & 2 \\\\ 2 & 1 \\\\ 1 & 1 \\end{bmatrix} $$\n",
    "\n",
    "We want to factorize this into two matrices $ W $ and $ H $ such that $ A \\approx WH $.\n",
    "\n",
    "#### Code in PyTorch\n",
    "\n",
    "Here's a simple implementation of matrix factorization in PyTorch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ikram.ali/miniconda3/envs/ml_notes/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 500, loss: 6.450703949667513e-05\n",
      "Iteration 1000, loss: 8.83535022921933e-09\n",
      "Iteration 1500, loss: 2.0268231537556858e-12\n",
      "Iteration 2000, loss: 9.698908343125368e-13\n",
      "Iteration 2500, loss: 5.773159728050814e-13\n",
      "Iteration 3000, loss: 1.7275070263167436e-13\n",
      "Iteration 3500, loss: 1.567634910770721e-13\n",
      "Iteration 4000, loss: 3.019806626980426e-14\n",
      "Iteration 4500, loss: 2.842170943040401e-14\n",
      "Iteration 5000, loss: 1.7763568394002505e-15\n",
      "Original Matrix A:\n",
      "tensor([[5., 3.],\n",
      "        [3., 2.],\n",
      "        [2., 1.],\n",
      "        [1., 1.]])\n",
      "\n",
      "Factorized Matrices W and H:\n",
      "tensor([[ 2.0369,  2.0575],\n",
      "        [ 1.4867,  0.9807],\n",
      "        [ 0.5502,  1.0768],\n",
      "        [ 0.9365, -0.0961]], requires_grad=True)\n",
      "tensor([[1.1957, 1.1052],\n",
      "        [1.2464, 0.3640]], requires_grad=True)\n",
      "\n",
      "Reconstructed Matrix A from W and H:\n",
      "tensor([[5.0000, 3.0000],\n",
      "        [3.0000, 2.0000],\n",
      "        [2.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Toy example matrix A\n",
    "A = torch.tensor([[5, 3],\n",
    "                  [3, 2],\n",
    "                  [2, 1],\n",
    "                  [1, 1]], dtype=torch.float32)\n",
    "\n",
    "# Dimensions\n",
    "m, n = A.size()\n",
    "k = 2  # Number of latent factors\n",
    "\n",
    "# Initialize W and H with random values\n",
    "W = torch.rand(m, k, requires_grad=True)\n",
    "H = torch.rand(k, n, requires_grad=True)\n",
    "\n",
    "# Define the learning rate and number of iterations\n",
    "learning_rate = 0.01\n",
    "num_iterations = 5000\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam([W, H], lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for iteration in range(num_iterations):\n",
    "    optimizer.zero_grad()\n",
    "    A_pred = torch.matmul(W, H)\n",
    "    loss = torch.nn.functional.mse_loss(A_pred, A)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (iteration+1) % 500 == 0:\n",
    "        print(f\"Iteration {iteration+1}, loss: {loss.item()}\")\n",
    "\n",
    "print(\"Original Matrix A:\")\n",
    "print(A)\n",
    "\n",
    "print(\"\\nFactorized Matrices W and H:\")\n",
    "print(W)\n",
    "print(H)\n",
    "\n",
    "print(\"\\nReconstructed Matrix A from W and H:\")\n",
    "print(torch.matmul(W, H))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of the Code\n",
    "\n",
    "1. **Initialization**: We initialize the matrices $ W $ and $ H $ with random values.\n",
    "2. **Optimization**: We use the Adam optimizer to update the values of $ W $ and $ H $.\n",
    "3. **Training Loop**: In each iteration, we compute the predicted matrix $ A_{pred} $ by multiplying $ W $ and $ H $. We then compute the mean squared error loss between $ A $ and $ A_{pred} $ and perform backpropagation to update $ W $ and $ H $.\n",
    "\n",
    "This implementation iteratively minimizes the reconstruction error, resulting in matrices $ W $ and $ H $ that approximate the original matrix $ A $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigen Decomposition\n",
    "\n",
    "Eigen decomposition (also known as spectral decomposition) is a fundamental concept in linear algebra used to decompose a square matrix into its eigenvalues and eigenvectors. It has numerous applications in various fields such as physics, engineering, computer science, and finance.\n",
    "\n",
    "### Why Eigen Decomposition Was Invented\n",
    "\n",
    "Eigen decomposition was developed to solve several important problems:\n",
    "\n",
    "1. **Understanding Matrix Behavior**: It provides insights into the properties of linear transformations represented by matrices.\n",
    "2. **Diagonalization**: It simplifies matrices into a diagonal form, making many matrix operations more efficient.\n",
    "3. **Solving Systems of Linear Equations**: It helps in solving differential equations and systems of linear equations.\n",
    "4. **Principal Component Analysis (PCA)**: It is crucial in data reduction techniques like PCA, where eigenvectors represent principal components of the data.\n",
    "\n",
    "### Problems Solved by Eigen Decomposition\n",
    "\n",
    "1. **Matrix Simplification**: By decomposing a matrix into eigenvalues and eigenvectors, complex matrix operations become simpler.\n",
    "2. **Data Analysis**: It is used in PCA to reduce the dimensionality of data while retaining its essential features.\n",
    "3. **Stability Analysis**: In control theory and dynamical systems, it helps analyze the stability of systems.\n",
    "4. **Quantum Mechanics**: It is used to solve the Schr√∂dinger equation and understand quantum states.\n",
    "\n",
    "### Mathematical Explanation\n",
    "\n",
    "Given a square matrix $ A $ of size $ n \\times n $, eigen decomposition involves finding a set of eigenvalues $ \\lambda $ and corresponding eigenvectors $ v $ such that:\n",
    "\n",
    "$$ Av = \\lambda v $$\n",
    "\n",
    "Where:\n",
    "- $ A $ is the original matrix.\n",
    "- $ \\lambda $ is an eigenvalue.\n",
    "- $ v $ is an eigenvector.\n",
    "\n",
    "#### Eigen Decomposition Formula\n",
    "\n",
    "If $ A $ is diagonalizable, it can be expressed as:\n",
    "\n",
    "$$ A = V \\Lambda V^{-1} $$\n",
    "\n",
    "Where:\n",
    "- $ V $ is a matrix whose columns are the eigenvectors of $ A $.\n",
    "- $ \\Lambda $ is a diagonal matrix whose diagonal elements are the eigenvalues of $ A $.\n",
    "- $ V^{-1} $ is the inverse of $ V $.\n",
    "\n",
    "### Toy Example\n",
    "\n",
    "Consider the following matrix $ A $:\n",
    "\n",
    "$$ A = \\begin{bmatrix} 4 & 1 \\\\ 2 & 3 \\end{bmatrix} $$\n",
    "\n",
    "We want to find the eigenvalues and eigenvectors of $ A $.\n",
    "\n",
    "### Finding Eigenvalues\n",
    "\n",
    "The eigenvalues are found by solving the characteristic equation:\n",
    "\n",
    "$$ \\det(A - \\lambda I) = 0 $$\n",
    "\n",
    "Where $ I $ is the identity matrix. For our matrix $ A $:\n",
    "\n",
    "$$ A - \\lambda I = \\begin{bmatrix} 4 - \\lambda & 1 \\\\ 2 & 3 - \\lambda \\end{bmatrix} $$\n",
    "\n",
    "The determinant of this matrix is:\n",
    "\n",
    "$$ (4 - \\lambda)(3 - \\lambda) - (2 \\cdot 1) = 0 $$\n",
    "$$ \\lambda^2 - 7\\lambda + 10 = 0 $$\n",
    "\n",
    "Solving this quadratic equation, we get:\n",
    "\n",
    "$$ \\lambda_1 = 5, \\lambda_2 = 2 $$\n",
    "\n",
    "### Finding Eigenvectors\n",
    "\n",
    "For each eigenvalue $ \\lambda $, we solve:\n",
    "\n",
    "$$ (A - \\lambda I)v = 0 $$\n",
    "\n",
    "For $ \\lambda = 5 $:\n",
    "\n",
    "$$ \\begin{bmatrix} 4 - 5 & 1 \\\\ 2 & 3 - 5 \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} = \\begin{bmatrix} -1 & 1 \\\\ 2 & -2 \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} = 0 $$\n",
    "\n",
    "This gives us the eigenvector $ v_1 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} $.\n",
    "\n",
    "For $ \\lambda = 2 $:\n",
    "\n",
    "$$ \\begin{bmatrix} 4 - 2 & 1 \\\\ 2 & 3 - 2 \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} = \\begin{bmatrix} 2 & 1 \\\\ 2 & 1 \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} = 0 $$\n",
    "\n",
    "This gives us the eigenvector $ v_2 = \\begin{bmatrix} -1 \\\\ 2 \\end{bmatrix} $.\n",
    "\n",
    "### Code in PyTorch\n",
    "#### Explanation of the Code\n",
    "\n",
    "1. **Initialization**: We define the matrix $ A $.\n",
    "2. **Eigen Decomposition**: We use `torch.linalg.eig` to compute the eigenvalues and eigenvectors.\n",
    "3. **Reconstruction**: We reconstruct the original matrix using the eigenvalues and eigenvectors to verify the decomposition.\n",
    "\n",
    "This implementation demonstrates the process of eigen decomposition and verifies that the original matrix can be reconstructed from its eigenvalues and eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Matrix A:\n",
      "tensor([[4., 1.],\n",
      "        [2., 3.]])\n",
      "\n",
      "Eigenvalues:\n",
      "tensor([5.+0.j, 2.+0.j])\n",
      "\n",
      "Eigenvectors:\n",
      "tensor([[ 0.7071+0.j, -0.4472+0.j],\n",
      "        [ 0.7071+0.j,  0.8944+0.j]])\n",
      "\n",
      "Reconstructed Matrix A:\n",
      "tensor([[4.0000+0.j, 1.0000+0.j],\n",
      "        [2.0000+0.j, 3.0000+0.j]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Toy example matrix A\n",
    "A = torch.tensor([[4.0, 1.0],\n",
    "                  [2.0, 3.0]])\n",
    "\n",
    "# Perform eigen decomposition\n",
    "eigenvalues, eigenvectors = torch.linalg.eig(A)\n",
    "\n",
    "print(\"Original Matrix A:\")\n",
    "print(A)\n",
    "\n",
    "print(\"\\nEigenvalues:\")\n",
    "print(eigenvalues)\n",
    "\n",
    "print(\"\\nEigenvectors:\")\n",
    "print(eigenvectors)\n",
    "\n",
    "# Reconstructing the matrix from eigenvalues and eigenvectors\n",
    "V = eigenvectors\n",
    "D = torch.diag(eigenvalues)\n",
    "V_inv = torch.linalg.inv(V)\n",
    "\n",
    "A_reconstructed = V @ D @ V_inv\n",
    "\n",
    "print(\"\\nReconstructed Matrix A:\")\n",
    "print(A_reconstructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_notes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
