{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non Parametric Models\n",
    "\n",
    "## KNN (K-Nearest Neighbors)\n",
    "\n",
    "The k-Nearest Neighbors (k-NN) algorithm is a type of supervised machine learning algorithm used for both classification and regression tasks. However, it's more commonly used for classification purposes. The \"k\" in k-NN represents a number specified by the user, and it refers to the number of nearest neighbors in the data that the algorithm will consider to make a prediction for a new data point.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **Choose the number of k and a distance metric**: First, you decide on the number of neighbors, \"k\", and the method for measuring distance between data points (common metrics include Euclidean, Manhattan, and Hamming distance).\n",
    "\n",
    "2. **Find the k-nearest neighbors**: For a given data point that you want to classify or predict its value, the algorithm identifies the k nearest data points in the training dataset based on the distance metric.\n",
    "\n",
    "3. **Make predictions**: \n",
    "   - **For classification**, the algorithm assigns the class to the new data point based on the majority vote of its k nearest neighbors.\n",
    "   - **For regression**, it predicts the value for the new point based on the average (or another aggregate measure) of the values of its k nearest neighbors.\n",
    "\n",
    "### Example\n",
    "\n",
    "Imagine you have a dataset of fruits, where each fruit is described by two features: weight and color (let's simplify color to a numerical value for the purpose of this example, where 1 = green, 2 = yellow, 3 = red), and you're trying to classify them as either \"Apple\" or \"Banana\".\n",
    "\n",
    "Now, you have a new fruit that you want to classify, and this fruit weighs 150 grams and has a color value of 1 (green).\n",
    "\n",
    "If you choose k=3 (looking at the three nearest neighbors), and the three closest fruits in your dataset to this new fruit are:\n",
    "- Fruit 1: 145 grams, color 1, labeled \"Apple\"\n",
    "- Fruit 2: 160 grams, color 2, labeled \"Apple\"\n",
    "- Fruit 3: 155 grams, color 1, labeled \"Banana\"\n",
    "\n",
    "Based on the majority vote among the nearest neighbors, the algorithm would classify the new fruit as an \"Apple\" because two out of three nearest neighbors are labeled as \"Apple\".\n",
    "\n",
    "This example simplifies the concept to make it easier to understand. In practice, the k-NN algorithm can handle datasets with many more features and more complex decision boundaries. The key takeaway is that the k-NN algorithm relies on the similarity of the nearest observations in the feature space to make predictions.\n",
    "\n",
    "\n",
    "```{image} https://cdn.mathpix.com/snip/images/5a3sBrsbGFOLPOY7XATS_ahpTBl-X9A9QnB7bs6UfYg.original.fullsize.png\n",
    ":alt: Knn\n",
    ":align: center\n",
    ":width: 90%\n",
    "```\n",
    "\n",
    "## Decision Trees\n",
    "\n",
    "```{image} https://cdn.mathpix.com/snip/images/Hhr4nUXPvrS7xn4jX0WeDJFYuaZyBZT_P1WBCr1W4_k.original.fullsize.png\n",
    ":alt: Decision Trees\n",
    ":align: center\n",
    ":width: 90%\n",
    "```\n",
    "\n",
    "\n",
    "Decision trees are a type of supervised learning algorithm used for both classification and regression tasks, though they are more commonly used for classification. They are called \"decision trees\" because the model uses a tree-like structure of decisions and their possible consequences, including chance event outcomes, resource costs, and utility.\n",
    "\n",
    "### How Decision Trees Work\n",
    "\n",
    "The algorithm divides the data into two or more homogeneous sets based on the most significant attributes making the groups as distinct as possible. It uses a method called \"recursive partitioning\" or \"splitting\" to do this, which starts at the top of the tree (the \"root\") and splits the data into subsets by making decisions based on feature values. This process is repeated on each derived subset in a recursive manner called recursive partitioning. The recursion is completed when the algorithm cannot make any further splits or when it reaches a predefined condition set by the user, such as a maximum tree depth or a minimum number of samples per leaf.\n",
    "\n",
    "### Components of Decision Trees\n",
    "\n",
    "- **Root Node**: Represents the entire dataset, which gets divided into two or more homogeneous sets.\n",
    "- **Splitting**: Process of dividing a node into two or more sub-nodes based on certain conditions.\n",
    "- **Decision Node**: After splitting, the sub-nodes become decision nodes, where further splits can occur.\n",
    "- **Leaf/Terminal Node**: Nodes that do not split further, representing the outcome or decision.\n",
    "- **Pruning**: Reducing the size of decision trees by removing parts of the tree that do not provide additional power to classify instances. This is done to make the tree simpler and to avoid overfitting.\n",
    "\n",
    "```{image} https://cdn.mathpix.com/snip/images/Vq66kx7sefZXguwCIqy1TM60U-zcpGlsFYILno-HBQY.original.fullsize.png\n",
    ":alt: Knn\n",
    ":align: center\n",
    ":width: 90%\n",
    "```\n",
    "\n",
    "### Criteria for Splitting\n",
    "\n",
    "Decision trees use various metrics to decide how to split the data at each step:\n",
    "- For classification tasks, commonly used metrics are Gini impurity, Entropy, and Classification Error.\n",
    "- For regression tasks, variance reduction is often used.\n",
    "\n",
    "### Example\n",
    "\n",
    "Imagine you want to decide on the activity for your weekend. The decision could depend on multiple factors such as the weather and whether you have company. A decision tree for this scenario might look something like this:\n",
    "\n",
    "- The root node starts with the question: \"Is it raining?\" \n",
    "    - If \"Yes\", the tree might direct you to a decision \"Stay in and read\".\n",
    "    - If \"No\", it then asks, \"Do you have company?\" \n",
    "        - If \"Yes\", the decision might be \"Go hiking\".\n",
    "        - If \"No\", the decision could be \"Visit a cafe\".\n",
    "\n",
    "This example simplifies the decision tree concept. In real-world data science tasks, decision trees consider many more variables and outcomes, and the decisions are based on quantitative data from the features of the dataset.\n",
    "\n",
    "### Advantages and Disadvantages\n",
    "\n",
    "**Advantages:**\n",
    "- Easy to understand and interpret.\n",
    "- Requires little data preparation.\n",
    "- Can handle both numerical and categorical data.\n",
    "- Can handle multi-output problems.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Prone to overfitting, especially with complex trees.\n",
    "- Can be unstable because small variations in the data might result in a completely different tree being generated.\n",
    "- Decision boundaries are linear, which may not accurately represent the data's actual structure.\n",
    "\n",
    "To combat overfitting, techniques like pruning (reducing the size of the tree), setting a maximum depth for the tree, and ensemble methods like Random Forests are often used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
