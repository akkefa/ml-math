{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "```{image} https://cdn.mathpix.com/snip/images/lAaxOlQRCuCwwXDTApLUeQgqYcx1irBn27FJv4XRKFQ.original.fullsize.png\n",
    ":alt: Logistic Regression\n",
    ":align: center\n",
    ":width: 90%\n",
    "```\n",
    "\n",
    "Logistic regression is a statistical method for analyzing a dataset in which there are one or more independent variables that determine an outcome. The outcome is measured with a dichotomous variable (in which there are only two possible outcomes). It's used extensively for binary classification problems, such as spam detection (spam or not spam), loan default (default or not), disease diagnosis (positive or negative), etc. Logistic regression predicts the probability that a given input belongs to a certain category.\n",
    "\n",
    "\n",
    "## Key Concepts of Logistic Regression:\n",
    "\n",
    "### Sigmoid Function:\n",
    " The core of logistic regression is the sigmoid function, which maps any real-valued number into a value between 0 and 1, making it suitable for probability estimation. The sigmoid function is defined as $\\sigma(z) = \\frac{1}{1 + e^{-z}}$, where $z$ is the input to the function, often $z = w^T x + b$, with $w$ being the weights, $x$ the input features, and $b$ the bias.\n",
    "\n",
    " ```{image} https://cdn.mathpix.com/snip/images/naz783Wg2Kw8ZAnQCfYL78cvzYPQDhj1EeHLkEWS30g.original.fullsize.png\n",
    ":alt: Sigmoid Function\n",
    ":align: center\n",
    ":width: 90%\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "```{image} https://cdn.mathpix.com/snip/images/rSUdzG9zCmzJjMn-49vhGnWPCgxYa54mRnru5jl-dyA.original.fullsize.png\n",
    ":alt: Sigmoid Function\n",
    ":align: center\n",
    ":width: 90%\n",
    "```\n",
    "\n",
    "  \n",
    "### Cost Function:\n",
    "\n",
    "The cost function used in logistic regression is the binary cross-entropy or log loss, which measures the performance of a classification model whose output is a probability value between 0 and 1. The cost function is minimized using optimization algorithms like gradient descent.\n",
    "  \n",
    "- **Threshold Decision**: The probability outcome from the sigmoid function is converted into a binary outcome via a threshold decision rule, usually 0.5 (if the sigmoid output is greater than or equal to 0.5, the outcome is classified as 1, otherwise as 0).\n",
    "\n",
    "### Logistic Regression in PyTorch:\n",
    "Hereâ€™s a simple example of how to implement logistic regression in PyTorch. PyTorch is a deep learning framework that provides a lot of flexibility and capabilities, including automatic differentiation which is handy for logistic regression.\n",
    "\n",
    "#### Step 1: Import Libraries\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "```\n",
    "\n",
    "#### Step 2: Create Dataset\n",
    "For simplicity, let's assume a binary classification task with some synthetic data.\n",
    "```python\n",
    "# Features [sample size, number of features]\n",
    "X = torch.tensor([[1, 2], [4, 5], [7, 8], [9, 10]], dtype=torch.float32)\n",
    "# Labels [sample size, 1]\n",
    "y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
    "```\n",
    "\n",
    "#### Step 3: Define the Model\n",
    "```python\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.sigmoid(self.linear(x))\n",
    "        return out\n",
    "```\n",
    "\n",
    "#### Step 4: Instantiate Model, Loss, and Optimizer\n",
    "```python\n",
    "input_size = 2\n",
    "num_classes = 1\n",
    "model = LogisticRegressionModel(input_size, num_classes)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "```\n",
    "\n",
    "#### Step 5: Train the Model\n",
    "```python\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "    \n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "```\n",
    "\n",
    "This code snippet demonstrates the essential parts of implementing logistic regression in PyTorch, including model definition, data preparation, loss computation, and the training loop. After training, the model's weights are adjusted to minimize the loss, making the model capable of predicting the probability that a new, unseen input belongs to a certain category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
