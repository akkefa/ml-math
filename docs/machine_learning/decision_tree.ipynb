{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "Decision trees are a type of supervised learning algorithm used for both classification and regression tasks, though they are more commonly used for classification. They are called \"decision trees\" because the model uses a tree-like structure of decisions and their possible consequences, including chance event outcomes, resource costs, and utility.\n",
    "\n",
    "```{image} https://cdn.mathpix.com/snip/images/Hhr4nUXPvrS7xn4jX0WeDJFYuaZyBZT_P1WBCr1W4_k.original.fullsize.png\n",
    ":alt: Decision Trees\n",
    ":align: center\n",
    ":width: 90%\n",
    "```\n",
    "\n",
    "## How Decision Trees Work\n",
    "\n",
    "The algorithm divides the data into two or more homogeneous sets based on the most significant attributes making the groups as distinct as possible. It uses a method called \"recursive partitioning\" or \"splitting\" to do this, which starts at the top of the tree (the \"root\") and splits the data into subsets by making decisions based on feature values. This process is repeated on each derived subset in a recursive manner called recursive partitioning. The recursion is completed when the algorithm cannot make any further splits or when it reaches a predefined condition set by the user, such as a maximum tree depth or a minimum number of samples per leaf.\n",
    "\n",
    "```{image} https://cdn.mathpix.com/snip/images/TnyrrmBOFzHW1cLDSclfhls4EIah5V3T39FNii0hGl0.original.fullsize.png\n",
    ":alt: Decision Trees\n",
    ":align: center\n",
    ":width: 90%\n",
    "```\n",
    "\n",
    "### Components of Decision Trees\n",
    "\n",
    "- **Root Node**: Represents the entire dataset, which gets divided into two or more homogeneous sets.\n",
    "- **Splitting**: Process of dividing a node into two or more sub-nodes based on certain conditions.\n",
    "- **Decision Node**: After splitting, the sub-nodes become decision nodes, where further splits can occur.\n",
    "- **Leaf/Terminal Node**: Nodes that do not split further, representing the outcome or decision.\n",
    "- **Pruning**: Reducing the size of decision trees by removing parts of the tree that do not provide additional power to classify instances. This is done to make the tree simpler and to avoid overfitting.\n",
    "\n",
    "```{image} https://cdn.mathpix.com/snip/images/Vq66kx7sefZXguwCIqy1TM60U-zcpGlsFYILno-HBQY.original.fullsize.png\n",
    ":alt: Knn\n",
    ":align: center\n",
    ":width: 90%\n",
    "```\n",
    "\n",
    "### Criteria for Splitting\n",
    "\n",
    "Decision trees use various metrics to decide how to split the data at each step:\n",
    "- For classification tasks, commonly used metrics are Gini impurity, Entropy, and Classification Error.\n",
    "- For regression tasks, variance reduction is often used.\n",
    "\n",
    "### Example\n",
    "\n",
    "Imagine you want to decide on the activity for your weekend. The decision could depend on multiple factors such as the weather and whether you have company. A decision tree for this scenario might look something like this:\n",
    "\n",
    "- The root node starts with the question: \"Is it raining?\" \n",
    "    - If \"Yes\", the tree might direct you to a decision \"Stay in and read\".\n",
    "    - If \"No\", it then asks, \"Do you have company?\" \n",
    "        - If \"Yes\", the decision might be \"Go hiking\".\n",
    "        - If \"No\", the decision could be \"Visit a cafe\".\n",
    "\n",
    "This example simplifies the decision tree concept. In real-world data science tasks, decision trees consider many more variables and outcomes, and the decisions are based on quantitative data from the features of the dataset.\n",
    "\n",
    "### Advantages and Disadvantages\n",
    "\n",
    "**Advantages:**\n",
    "- Easy to understand and interpret.\n",
    "- Requires little data preparation.\n",
    "- Can handle both numerical and categorical data.\n",
    "- Can handle multi-output problems.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Prone to overfitting, especially with complex trees.\n",
    "- Can be unstable because small variations in the data might result in a completely different tree being generated.\n",
    "- Decision boundaries are linear, which may not accurately represent the data's actual structure.\n",
    "\n",
    "To combat overfitting, techniques like pruning (reducing the size of the tree), setting a maximum depth for the tree, and ensemble methods like Random Forests are often used.\n",
    "\n",
    "## Decision Tree Regressor\n",
    "A Decision Tree Regressor is a type of machine learning model used for predicting continuous values, unlike its counterpart, the Decision Tree Classifier, which predicts categorical outcomes. It works by breaking down a dataset into smaller subsets while simultaneously developing an associated decision tree. The final result is a tree with decision nodes and leaf nodes.\n",
    "\n",
    "The Decision Tree Regressor uses the Mean Squared Error (MSE) as a measure to decide on the best split at each decision node. MSE is a popular metric used to evaluate the performance of a regression model, indicating the average squared difference between the observed actual outturns and the predictions made by the model. The goal of the regressor is to minimize the MSE at each step of building the tree.\n",
    "\n",
    "### How it Works Using MSE\n",
    "\n",
    "1. **Starting at the Root**: The entire dataset is considered as the root.\n",
    "2. **Best Split Decision**: To decide on a split, it calculates the MSE for every possible split in every feature and chooses the one that results in the lowest MSE. This split is the one that, if used to split the dataset into two groups, would result in the most similar responses within each group.\n",
    "3. **Recursion on Subsets**: This process of finding the best split is then recursively applied to each resulting subset. The recursion is completed when the algorithm reaches a predetermined stopping criterion, such as a maximum depth of the tree or a minimum number of samples required to split a node further.\n",
    "4. **Prediction**: For a prediction, the input features of a new data point are fed through the decision tree. The path followed by the data point through the tree leads to a leaf node. The average of the values in this leaf node is used as the prediction.\n",
    "\n",
    "```{image} https://cdn.mathpix.com/snip/images/SX0hVPA_he1R54UP1hajzXcda2lWuXlqpSFPk89WX0M.original.fullsize.png\n",
    ":alt: Decision Tree\n",
    ":align: center\n",
    ":width: 80%\n",
    "```\n",
    "---\n",
    "\n",
    "```{image} https://cdn.mathpix.com/snip/images/ZlP7-kSqXm9bkBqv5Wb51JKMgQC-LhitRem27EIHcYs.original.fullsize.png\n",
    ":alt: Decision Tree\n",
    ":align: center\n",
    ":width: 80%\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "```{image} https://cdn.mathpix.com/snip/images/PF7mlEk6tZHSX8YYJ79JD_4T7owgaCL_3XHwni9npjg.original.fullsize.png\n",
    ":alt: Decision Tree\n",
    ":align: center\n",
    ":width: 80%\n",
    "```\n",
    "\n",
    "### Example\n",
    "\n",
    "Imagine we are using a dataset of houses, where our features include the number of bedrooms, the number of bathrooms, square footage, and the year built, and our target variable is the house price.\n",
    "\n",
    "1. **Root**: Initially, the entire dataset is the root.\n",
    "2. **Best Split Calculation**: The algorithm evaluates all features and their possible values to find the split that would result in subsets with the most similar house prices (lowest MSE). Suppose the best initial split divides the dataset into houses with less than 2 bathrooms and houses with 2 or more bathrooms.\n",
    "3. **Recursive Splitting**: This process continues, with each subset being split on features and feature values that minimize the MSE within each resulting subset. For instance, within the subset of houses with less than 2 bathrooms, the next split might be on the number of bedrooms.\n",
    "4. **Stopping Criterion Reached**: Eventually, when the stopping criteria are met (for example, a maximum depth of the tree), the splitting stops.\n",
    "5. **Making Predictions**: To predict the price of a new house, we would input its features into the decision tree. The house would follow a path down the tree determined by its features until it reaches a leaf node. The prediction would be the average price of the houses in that leaf node.\n",
    "\n",
    "This example simplifies the complexity involved in building a decision tree regressor but gives an outline of how MSE is used to create a model that can predict continuous outcomes like house prices.\n",
    "\n",
    "```{image} https://cdn.mathpix.com/snip/images/bE3EaEGRPMjmp_y36LHCt8eMgv31K5AY-hpV3U3TpyM.original.fullsize.png\n",
    ":alt: Decision Tree\n",
    ":align: center\n",
    ":width: 80%\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "```{image} https://cdn.mathpix.com/snip/images/7f1Ex-fm3pieQJhP7RZ8L8vqyHHhs2AFLPlHl0LgElg.original.fullsize.png\n",
    ":alt: Decision Tree\n",
    ":align: center\n",
    ":width: 80%\n",
    "```\n",
    "\n",
    "## Decision Tree Classifier\n",
    "Decision Tree Classifier is similar to Decision Tree Regressor, with the difference that it classifies the target variable.\n",
    "\n",
    "A Decision Tree Classifier is a flowchart-like structure in which each internal node represents a \"test\" on an attribute (e.g., \"Is the animal you're thinking of larger than a breadbox?\"), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes). The paths from root to leaf represent classification rules.\n",
    "\n",
    "### How it works:\n",
    "\n",
    "1. **Starting at the root:** Based on the data (features of animals in our analogy), the algorithm chooses the most significant feature to split the data into groups. This is usually done using measures like Gini impurity or information gain to determine which feature brings us closer to a decision.\n",
    "2. **Splitting:** This process is repeated for each child node (e.g., if the first question splits the animals into \"larger than a breadbox\" and \"not larger than a breadbox,\" each of those groups is then split again based on another feature), creating the tree.\n",
    "3. **Stopping Criteria:** The tree stops growing when it meets a stopping criterion, like when it can no longer reduce uncertainty or when it reaches a specified depth.\n",
    "4. **Decision Making:** Once built, you can use the tree to classify new cases (predict the class of a new animal) by following the decisions in the tree based on the features of the new case.\n",
    "\n",
    "### Gini Impurity: Simplified\n",
    "\n",
    "$$\n",
    "\n",
    "H(X_m) = \\sum_{k} p_{mk} (1 - p_{mk})\n",
    "\n",
    "$$\n",
    "\n",
    "The Gini impurity formula you've provided is a mathematical way to quantify how \"pure\" a set (usually a set of data points in a decision tree node) is. It tells us what the chance is that a randomly chosen element from the set is incorrectly labeled if it was randomly labeled according to the distribution of classes in the set. Let's break it down:\n",
    "\n",
    "- $ H(X_m) $ is the Gini impurity of a set $ m $.\n",
    "- $ p_{mk} $ is the proportion (or probability) of class $ k $ in the set $ m $\n",
    "\n",
    "The formula sums up the product of the probability of each class with the probability of not being that class (which is $ 1 - p_{mk} $). This product gives a measure of the probability of misclassification for each class. By summing over all classes, the Gini impurity considers the probability of misclassification regardless of what class we're talking about.\n",
    "\n",
    "Let's go through a step-by-step example with a tangible dataset.\n",
    "\n",
    "#### Example Dataset:\n",
    "Imagine we have a small dataset of 10 animals, with two features: \"Can Fly\" (Yes or No) and \"Has Fins\" (Yes or No). We want to classify them into two classes: \"Bird\" or \"Fish\".\n",
    "\n",
    "Here are the animals:\n",
    "\n",
    "- 4 are birds that can fly.\n",
    "- 2 are birds that cannot fly (perhaps penguins).\n",
    "- 3 are fish that have fins.\n",
    "- 1 is a fish that does not have fins (maybe an eel).\n",
    "\n",
    "#### Step 1: Calculate Class Proportions\n",
    "We first need to calculate the proportions of each class in the set.\n",
    "\n",
    "- Birds: $ p_{bird} = \\frac{4 + 2}{10} = \\frac{6}{10} = 0.6 $\n",
    "- Fish: $ p_{fish} = \\frac{3 + 1}{10} = \\frac{4}{10} = 0.4 $\n",
    "\n",
    "#### Step 2: Plug Proportions Into the Formula\n",
    "Now we use the Gini formula.\n",
    "\n",
    "- Gini for birds: $ p_{bird} \\times (1 - p_{bird}) = 0.6 \\times (1 - 0.6) = 0.6 \\times 0.4 = 0.24 $\n",
    "- Gini for fish: $ p_{fish} \\times (1 - p_{fish}) = 0.4 \\times (1 - 0.4) = 0.4 \\times 0.6 = 0.24 $\n",
    "\n",
    "#### Step 3: Sum the Gini for All Classes\n",
    "The Gini impurity for the set is the sum of the Gini for all classes.\n",
    "\n",
    "- Total Gini impurity: $ H(X_m) = 0.24 + 0.24 = 0.48 $\n",
    "\n",
    "This Gini impurity value of 0.48 is relatively high, indicating that the set is quite mixed (if it were 0, the set would be perfectly pure).\n",
    "\n",
    "#### Step 4: Interpretation\n",
    "A Gini impurity of 0.48 means that if we pick a random animal from this dataset and then randomly assign a label based on the distribution of the classes (60% chance of being labeled as a bird and 40% as a fish), there's a 48% chance of mislabeling the animal.\n",
    "\n",
    "The goal in a decision tree is to create splits that result in subsets with lower Gini impurity scores compared to the parent node. A perfect split would be one that results in nodes with a Gini impurity of 0, meaning all elements in each node are from a single class.\n",
    "\n",
    "- **Gini Impurity = 1 - sum (probability of each class)^2**\n",
    "\n",
    "The best (lowest) Gini impurity is 0, where all elements belong to a single class (pure). The worst (highest) Gini impurity is 0.5 in a binary classification, where the dataset is evenly split between two classes (completely mixed).\n",
    "\n",
    "#### Example:\n",
    "\n",
    "Let's say we have a dataset of 10 animals, 6 are fish and 4 are birds.\n",
    "\n",
    "- The probability of picking a fish randomly is 6/10, and the probability of picking a bird is 4/10.\n",
    "- Gini Impurity = 1 - ( (6/10)^2 + (4/10)^2 ) = 1 - ( 0.36 + 0.16 ) = 1 - 0.52 = 0.48\n",
    "\n",
    "This Gini score tells us how often we would be wrong if we randomly assigned a label to an animal in this group based on the distribution of classes. A lower score is better, indicating less impurity.\n",
    "\n",
    "```{image} https://cdn.mathpix.com/snip/images/Oz5NKPBFVmr_MygKCK3rl7MuXvUd4SbRme-yigHPmI8.original.fullsize.png\n",
    ":alt: Decision Tree\n",
    ":align: center\n",
    ":width: 80%\n",
    "```\n",
    "\n",
    "```{image} https://cdn.mathpix.com/snip/images/EfvuPbzSJjY81CfzK9yacKEcctVDrrFaDVlTIMMnHj4.original.fullsize.png\n",
    ":alt: Decision Tree\n",
    ":align: center\n",
    ":width: 80%\n",
    "```\n",
    "---\n",
    "\n",
    "```{image} https://cdn.mathpix.com/snip/images/466vsJVhgFUagGxC-lAAhG_8makpw-Tc0oUw440obQA.original.fullsize.png\n",
    ":alt: Decision Tree\n",
    ":align: center\n",
    ":width: 80%\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "We'll apply the Gini impurity formula:\n",
    "\n",
    "$$ Gini: H(X_m) = \\sum_{k} p_{mk} (1 - p_{mk}) $$\n",
    "\n",
    "For the given probabilities of the two classes:\n",
    "\n",
    "$ p_{\\text{class1}} = 0.77777777777 $\n",
    "$ p_{\\text{class2}} = 0.22222222222 $\n",
    "\n",
    "The Gini impurity for each class is calculated as:\n",
    "\n",
    "For class 1:\n",
    "$ H(X_{m1}) = p_{\\text{class1}} (1 - p_{\\text{class1}}) = 0.77777777777 \\times (1 - 0.77777777777) $\n",
    "\n",
    "For class 2:\n",
    "$ H(X_{m2}) = p_{\\text{class2}} (1 - p_{\\text{class2}}) = 0.22222222222 \\times (1 - 0.22222222222) $\n",
    "\n",
    "Adding the Gini impurity of both classes:\n",
    "\n",
    "$$\n",
    "\n",
    "H(X_m) &= H(X_{m1}) + H(X_{m2}) \\\\\n",
    "H(X_m) &= 0.77777777777 \\times 0.22222222223 + 0.22222222222 \\times 0.77777777778  \\\\\n",
    "H(X_m) &= 0.17283950617 + 0.17283950616 \\\\\n",
    "H(X_m) &= 0.34567901233\n",
    "\n",
    "$$\n",
    "\n",
    "The Gini impurity for the binary classification with the given class probabilities is approximately 0.3457.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
